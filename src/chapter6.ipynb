{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Tokenizersåº“](https://huggingface.co/learn/nlp-course/zh-CN/chapter6)\n",
    "å½“æˆ‘ä»¬éœ€è¦å¾®è°ƒæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ä¸æ¨¡å‹é¢„è®­ç»ƒç›¸åŒçš„`tokenizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åŸºäºå·²æœ‰çš„ tokenizer è®­ç»ƒæ–°çš„ tokenizer\n",
    "å¤§å¤šæ•°`Transformer æ¨¡å‹`ä½¿ç”¨`å­è¯åˆ†è¯ç®—æ³•`ã€‚ä¸ºäº†æ‰¾åˆ°è¯­æ–™åº“ä¸­çš„å¸¸è§å­è¯ï¼Œ`tokenizer`éœ€è¦æ·±å…¥ç»Ÿè®¡è¯­æ–™åº“ä¸­çš„æ‰€æœ‰æ–‡æœ¬â€”â€”è¿™ä¸ªè¿‡ç¨‹æˆ‘ä»¬ç§°ä¹‹ä¸º`è®­ç»ƒï¼ˆtrainingï¼‰`ã€‚å…·ä½“çš„è®­ç»ƒè§„åˆ™å–å†³äºä½¿ç”¨çš„`tokenizer`ç±»å‹ã€‚\n",
    "> è®­ç»ƒ tokenizer ä¸è®­ç»ƒæ¨¡å‹ä¸åŒï¼æ¨¡å‹è®­ç»ƒä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™ä½¿æ¯ä¸ª batch çš„ loss å°ä¸€ç‚¹ã€‚å®ƒæœ¬è´¨ä¸Šæ˜¯éšæœºçš„ï¼ˆè¿™æ„å‘³ç€åœ¨å³ä½¿ä¸¤æ¬¡è®­ç»ƒçš„å‚æ•°å’Œç®—æ³•å®Œå…¨ç›¸åŒï¼Œä½ ä¹Ÿå¿…é¡»è®¾ç½®ä¸€äº›éšæœºæ•°ç§å­æ‰èƒ½è·å¾—ç›¸åŒçš„ç»“æœï¼‰ã€‚è®­ç»ƒ tokenizer æ˜¯ä¸€ä¸ªç»Ÿè®¡è¿‡ç¨‹ï¼Œå®ƒè¯•å›¾ç¡®å®šå“ªäº›å­è¯æœ€é€‚åˆä¸ºç»™å®šçš„è¯­æ–™åº“é€‰æ‹©ï¼Œç¡®å®šçš„è¿‡ç¨‹å–å†³äºåˆ†è¯ç®—æ³•ã€‚å®ƒæ˜¯ç¡®å®šæ€§çš„ï¼Œè¿™æ„å‘³ç€åœ¨ç›¸åŒçš„è¯­æ–™åº“ä¸Šä½¿ç”¨ç›¸åŒçš„ç®—æ³•è¿›è¡Œè®­ç»ƒæ—¶ï¼Œå¾—åˆ°çš„ç»“æœæ€»æ˜¯ç›¸åŒçš„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1 å‡†å¤‡è¯­æ–™åº“\n",
    "from datasets import load_dataset\n",
    "\n",
    "# # ä¸‹è½½ä¸ä¸‹æ¥æ•°æ®çš„æ—¶å€™å¯ä»¥å¢åŠ é­”æ³•\n",
    "# import os\n",
    "# os.environ[\"http_proxy\"] = \"http://127.0.0.1:port\"\n",
    "# os.environ[\"https_proxy\"] = \"http://127.0.0.1:port\"\n",
    "\n",
    "# # åŠ è½½æ•°æ®é›†\n",
    "raw_datasets = load_dataset(\"code_search_net\", \"python\", trust_remote_code=True)\n",
    "print(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æˆ‘ä»¬ä»…ä½¿ç”¨ whole_func_string åˆ—æ¥è®­ç»ƒæˆ‘ä»¬çš„ tokenizer \n",
    "# æˆ‘ä»¬å¯ä»¥é€šè¿‡ç´¢å¼•æ¥æŸ¥çœ‹å…¶ä¸­ä¸€ä¸ªå‡½æ•°çš„ç¤ºä¾‹\n",
    "print(raw_datasets[\"train\"][123456][\"whole_func_string\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºè¿­ä»£å™¨\n",
    "# å°†æ•°æ®é›†è½¬æ¢ä¸ºä¸€ä¸ªæ–‡æœ¬åˆ—è¡¨çš„iteratorï¼ˆè¿­ä»£å™¨ï¼‰ï¼Œè¿™æ ·å°±å¯ä»¥ä»¥batchçš„æ–¹å¼è¿›è¡Œè®­ç»ƒäº†\n",
    "# ä½¿ç”¨è¿­ä»£å™¨å¯ä»¥ä¸æŠŠæ‰€æœ‰å†…å®¹éƒ½åŠ è½½åˆ°å†…å­˜ä¸­\n",
    "# é€šè¿‡ python çš„ generator æ¥å®ç°\n",
    "## version 1 åŸºäºåˆ—è¡¨çš„ generator\n",
    "def get_training_corpus():\n",
    "    return (\n",
    "        raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
    "        for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    "    )\n",
    "\n",
    "\n",
    "training_corpus = get_training_corpus()\n",
    "print(training_corpus)\n",
    "\n",
    "## version 2 åŸºäº yield çš„ generator\n",
    "def get_training_corpus_v2():\n",
    "    dataset = raw_datasets[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"whole_func_string\"]\n",
    "    \n",
    "training_corpus_v2 = get_training_corpus_v2()\n",
    "print(training_corpus_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step2 è®­ç»ƒä¸€ä¸ªæ–°çš„ tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "# ä½¿ç”¨æ—§çš„ tokenizerï¼Œè®­ç»ƒä¸€ä¸ªæ–°çš„ tokenizer\n",
    "# æ–°çš„ tokenizer å°†ä¸ GPT-2 å®Œå…¨ç›¸åŒï¼Œå”¯ä¸€çš„åŒºåˆ«æ˜¯è¯æ±‡è¡¨ï¼Œè¿™å°†ç”±æˆ‘ä»¬çš„è¯­æ–™åº“é€šè¿‡è®­ç»ƒæ¥é‡æ–°ç¡®å®šã€‚\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "example = '''def add_numbers(a, b):\n",
    "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "    return a + b'''\n",
    "\n",
    "tokens = old_tokenizer.tokenize(example)\n",
    "# å±•ç¤ºåœ¨æ—§çš„ tokenizer ä¸‹çš„ token\n",
    "print(tokens)\n",
    "# ä½¿ç”¨code_search_netä¸­çš„Pythonæ•°æ®é›†ï¼Œè®­ç»ƒä¸€ä¸ªæ–°çš„ tokenizer ï¼Œè¯æ±‡è¡¨å¤§å°ä¸º52000\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)\n",
    "tokens = tokenizer.tokenize(example)\n",
    "print(tokens)\n",
    "print(len(tokens))\n",
    "print(len(old_tokenizer.tokenize(example)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token-classification\n",
    "`Token-classification`æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„æ ¸å¿ƒä»»åŠ¡ï¼Œæ—¨åœ¨ä¸ºæ–‡æœ¬ä¸­çš„æ¯ä¸ª tokenï¼ˆå¦‚å•è¯ã€å­è¯æˆ–å­—ç¬¦ï¼‰åˆ†é…ä¸€ä¸ªç‰¹å®šçš„æ ‡ç­¾ã€‚å®ƒçš„æ ¸å¿ƒç›®æ ‡æ˜¯é€šè¿‡å¯¹æ–‡æœ¬çš„ç»†ç²’åº¦åˆ†æï¼Œæå–ç»“æ„åŒ–ä¿¡æ¯æˆ–ç†è§£è¯­è¨€å•å…ƒçš„è¯­ä¹‰è§’è‰²ï¼Œå¹¿æ³›åº”ç”¨äºå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ã€åˆ†è¯ã€è¯æ€§æ ‡æ³¨ç­‰åœºæ™¯ã€‚    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨pipelineï¼Œé»˜è®¤åŸºäº dbmdz/bert-large-cased-finetuned-conll03-english æ¨¡å‹å¯¹å¥å­è¿›è¡Œ NER\n",
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\"token-classification\")\n",
    "print(token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\"))\n",
    "# å¯ä»¥ä½¿ç”¨å°†åŒä¸€å®ä½“çš„ token ç»„åˆåœ¨ä¸€èµ·\n",
    "## aggregation_strategy é€šè¿‡è®¾ç½®ï¼Œå¯ä»¥è®¾ç½®ç»„åˆåå®ä½“è®¡ç®—çš„ç­–ç•¥\n",
    "token_classifier = pipeline(\"token-classification\", aggregation_strategy=\"simple\")\n",
    "print(token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è‡ªå·±æ„å»ºpipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "inputs = tokenizer(example, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "# è¾“å…¥çš„tokençš„ç»´åº¦\n",
    "print(inputs[\"input_ids\"].shape)\n",
    "# è¾“å‡ºçš„ç»´åº¦\n",
    "print(outputs.logits.shape)\n",
    "# æ‰“å°ç´¢å¼•åˆ°æ ‡ç­¾çš„æ˜ å°„\n",
    "print(model.config.id2label)\n",
    "\n",
    "import torch\n",
    "# ä½¿ç”¨ softmax å‡½æ•°å°†è¿™äº› logits è½¬åŒ–ä¸ºæ¦‚ç‡ï¼Œå¹¶å– argmax æ¥å¾—åˆ°é¢„æµ‹\n",
    "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()\n",
    "predictions = outputs.logits.argmax(dim=-1)[0].tolist()\n",
    "print(predictions)\n",
    "\n",
    "# æ ¼å¼åŒ–è¾“å‡º token çš„å¾—åˆ†å’Œæ ‡ç­¾\n",
    "results = []\n",
    "# è·å–åç§»æ˜ å°„\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        start, end = offsets[idx]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity\": label,\n",
    "                \"score\": probabilities[idx][pred],\n",
    "                \"word\": tokens[idx],\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(results)\n",
    "\n",
    "# æ ¼å¼åŒ–è¾“å‡ºç»„åˆå token çš„å¾—åˆ†å’Œæ ‡ç­¾\n",
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "idx = 0\n",
    "while idx < len(predictions):\n",
    "    pred = predictions[idx]\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        # åˆ é™¤ B- æˆ–è€… I-\n",
    "        label = label[2:]\n",
    "        start, _ = offsets[idx]\n",
    "\n",
    "        # è·å–æ‰€æœ‰æ ‡æœ‰ I æ ‡ç­¾çš„token\n",
    "        all_scores = []\n",
    "        while (\n",
    "            idx < len(predictions)\n",
    "            and model.config.id2label[predictions[idx]] == f\"I-{label}\"\n",
    "        ):\n",
    "            all_scores.append(probabilities[idx][pred])\n",
    "            _, end = offsets[idx]\n",
    "            idx += 1\n",
    "\n",
    "        # åˆ†æ•°æ˜¯è¯¥åˆ†ç»„å®ä½“ä¸­æ‰€æœ‰tokenåˆ†æ•°çš„å¹³å‡å€¼\n",
    "        score = np.mean(all_scores).item()\n",
    "        word = example[start:end]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity_group\": label,\n",
    "                \"score\": score,\n",
    "                \"word\": word,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "    idx += 1\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question-answering\n",
    "question-answering pipeline ç”¨äºæ‰§è¡Œé—®ç­”ä»»åŠ¡ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒç”¨äºä»ç»™å®šçš„æ–‡æœ¬ä¸­æå–ç­”æ¡ˆï¼ŒåŸºäºä¸€ä¸ªé¢„è®­ç»ƒçš„æ¨¡å‹ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "context = \"\"\"\n",
    "ğŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch, and TensorFlow â€” with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "long_context = \"\"\"\n",
    "ğŸ¤— Transformers: State of the Art NLP\n",
    "\n",
    "ğŸ¤— Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\n",
    "question answering, summarization, translation, text generation and more in over 100 languages.\n",
    "Its aim is to make cutting-edge NLP easier to use for everyone.\n",
    "\n",
    "ğŸ¤— Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\n",
    "then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\n",
    "can be modified to enable quick research experiments.\n",
    "\n",
    "Why should I use transformers?\n",
    "\n",
    "1. Easy-to-use state-of-the-art models:\n",
    "  - High performance on NLU and NLG tasks.\n",
    "  - Low barrier to entry for educators and practitioners.\n",
    "  - Few user-facing abstractions with just three classes to learn.\n",
    "  - A unified API for using all our pretrained models.\n",
    "  - Lower compute costs, smaller carbon footprint:\n",
    "\n",
    "2. Researchers can share trained models instead of always retraining.\n",
    "  - Practitioners can reduce compute time and production costs.\n",
    "  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n",
    "\n",
    "3. Choose the right framework for every part of a model's lifetime:\n",
    "  - Train state-of-the-art models in 3 lines of code.\n",
    "  - Move a single model between TF2.0/PyTorch frameworks at will.\n",
    "  - Seamlessly pick the right framework for training, evaluation and production.\n",
    "\n",
    "4. Easily customize a model or an example to your needs:\n",
    "  - We provide examples for each architecture to reproduce the results published by its original authors.\n",
    "  - Model internals are exposed as consistently as possible.\n",
    "  - Model files can be used independently of the library for quick experiments.\n",
    "\n",
    "ğŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch and TensorFlow â€” with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "\n",
    "question = \"Which deep learning libraries back ğŸ¤— Transformers?\"\n",
    "print(question_answerer(question=question, context=context))\n",
    "# é•¿æ–‡æœ¬ä¹Ÿèƒ½å¤„ç†\n",
    "print(question_answerer(question=question, context=long_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è‡ªå·±æ„å»ºpipeline\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "model_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "# è¾“å…¥æ ¼å¼æ˜¯ï¼š[CLS] question [SEP] context [SEP] \n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "# é—®ç­”æ¨¡å‹è®­ç»ƒçš„ç›®æ ‡æ˜¯æ¥é¢„æµ‹ç­”æ¡ˆå¼€å§‹çš„ token çš„ç´¢å¼•å’Œç­”æ¡ˆç»“æŸçš„ token çš„ç´¢å¼•ã€‚\n",
    "# æ‰€ä»¥è¿”å›å€¼æ˜¯ä¸¤ä¸ªlogitsï¼šä¸€ä¸ªå¯¹åº”äºç­”æ¡ˆçš„å¼€å§‹ token çš„ logitsï¼Œå¦ä¸€ä¸ªå¯¹åº”äºç­”æ¡ˆçš„ç»“æŸ token çš„ logitsã€‚\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(start_logits.shape, end_logits.shape)\n",
    "\n",
    "# ä½¿ç”¨ softmax å‡½æ•°å°†è¿™äº› logits è½¬åŒ–ä¸ºæ¦‚ç‡ï¼Œå¹¶å– argmax æ¥å¾—åˆ°é¢„æµ‹\n",
    "\"\"\"\n",
    "æ³¨æ„ï¼š\n",
    "ç”±äºè¾“å…¥æ ¼å¼æ˜¯ [CLS] question [SEP] context [SEP] ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å±è”½ question çš„ tokens ä»¥åŠ [SEP] token ã€‚\n",
    "ä¸è¿‡ï¼Œæˆ‘ä»¬å°†ä¿ç•™ [CLS] ï¼Œå› ä¸ºæŸäº›æ¨¡å‹ä½¿ç”¨å®ƒæ¥è¡¨ç¤ºç­”æ¡ˆä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "sequence_ids = inputs.sequence_ids()\n",
    "# å±è”½é™¤ context ä¹‹å¤–çš„æ‰€æœ‰å†…å®¹\n",
    "mask = [i != 1 for i in sequence_ids]\n",
    "# ä¸å±è”½ [CLS] token\n",
    "mask[0] = False\n",
    "mask = torch.tensor(mask)[None]\n",
    "print(mask)\n",
    "# å°†æƒ³è¦å±è”½çš„ logits æ›¿æ¢ä¸ºä¸€ä¸ªå¤§çš„è´Ÿæ•°\n",
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000\n",
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]\n",
    "print(start_probabilities.shape, end_probabilities.shape)\n",
    "\n",
    "# åœ¨æ»¡è¶³ start_index <= end_index çš„å‰æä¸‹ï¼Œè®¡ç®—æ¯ä¸ªå¯èƒ½çš„ start_index å’Œ end_index çš„æ¦‚ç‡ï¼Œ\n",
    "# ç„¶åå–æ¦‚ç‡æœ€é«˜çš„ (start_index, end_index) å…ƒç»„ã€‚\n",
    "# start_probabilities[start_index] Ã— end_probabilities[end_index] å°±æ˜¯è¿™ä¸ª (start_index, end_index) å…ƒç»„çš„æ¦‚ç‡ã€‚\n",
    "scores = start_probabilities[:, None] * end_probabilities[None, :]\n",
    "print(scores.shape)\n",
    "import numpy as np\n",
    "# è·å–ä¸Šä¸‰è§’éƒ¨åˆ†\n",
    "scores = torch.triu(scores)\n",
    "\n",
    "# argmax å°†è¿”å›å±•å¹³ï¼ˆflattenedï¼‰åå¼ é‡ä¸­çš„ç´¢å¼•\n",
    "max_index = scores.argmax().item()\n",
    "start_index = max_index // scores.shape[1]\n",
    "end_index = max_index % scores.shape[1]\n",
    "print(start_index, end_index)\n",
    "print(scores[start_index, end_index])\n",
    "\n",
    "# æœ‰äº†start_index å’Œ end_indexï¼Œå¯ä»¥ä» context ä¸­æå–ç­”æ¡ˆ\n",
    "inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "start_char, _ = offsets[start_index]\n",
    "_, end_char = offsets[end_index]\n",
    "answer = context[start_char:end_char]\n",
    "# æ ¼å¼åŒ–è¾“å‡º\n",
    "result = {\n",
    "    \"answer\": answer,\n",
    "    \"start\": start_char,\n",
    "    \"end\": end_char,\n",
    "    \"score\": scores[start_index, end_index],\n",
    "}\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤„ç†é•¿æ–‡æœ¬\n",
    "\"\"\"\n",
    "question-answering pipeline \n",
    "é€šè¿‡è®¾ç½®max_lengthï¼Œé™åˆ¶tokensæ•°é‡çš„æœ€å¤§å€¼ä¸º384\n",
    "å½“è¶…è¿‡è¿™ä¸ªé™åˆ¶æ—¶ï¼Œæˆ‘ä»¬éœ€è¦å°†é•¿æ–‡æœ¬åˆ†å‰²æˆå¤šä¸ªå°æ®µï¼Œç„¶ååœ¨å°æ®µä¸­å¯»æ‰¾ç­”æ¡ˆ\n",
    "ä¸ºäº†é˜²æ­¢ç­”æ¡ˆè¢«åˆ†å‰²ï¼Œæˆ‘ä»¬éœ€è¦åœ¨å„å—ä¹‹é—´é‡å ä¸€äº›å†…å®¹\n",
    "\n",
    "é€šè¿‡ä½¿ç”¨ä»¥ä¸‹å‚æ•°å®ç°ï¼š\n",
    "1. return_overflowing_tokens=Trueï¼Œå½“è¾“å…¥æ–‡æœ¬é•¿åº¦è¶…è¿‡æ¨¡å‹æ”¯æŒçš„æœ€å¤§é•¿åº¦æ—¶ï¼Œ\n",
    "   è¯¥å‚æ•°ä¼šå°†æ–‡æœ¬åˆ†å‰²æˆå¤šä¸ªå­ç‰‡æ®µï¼ˆchunksï¼‰ï¼Œå¹¶è¿”å›æ‰€æœ‰æº¢å‡ºç‰‡æ®µã€‚\n",
    "2. stride å®šä¹‰åˆ†å‰²åçš„å­ç‰‡æ®µä¹‹é—´çš„é‡å tokenæ•°é‡ã€‚\n",
    "\"\"\"\n",
    "inputs = tokenizer(\n",
    "    question,\n",
    "    long_context,\n",
    "    stride=128,\n",
    "    max_length=384,\n",
    "    padding=\"longest\",\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "_ = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "offsets = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "inputs = inputs.convert_to_tensors(\"pt\")\n",
    "print(inputs[\"input_ids\"].shape)\n",
    "# sequence_ids ç”¨äºæ ‡è¯†è¾“å…¥ä¸­ä¸åŒå¥å­çš„è¾¹ç•Œï¼ˆå¸¸è§äºå¤šåºåˆ—è¾“å…¥åœºæ™¯ï¼‰ã€‚\n",
    "# ä¾‹å¦‚åœ¨é—®ç­”ä»»åŠ¡ä¸­ï¼Œè¾“å…¥ç”±é—®é¢˜ï¼ˆæ ‡è®°ä¸º0ï¼‰å’Œä¸Šä¸‹æ–‡ï¼ˆæ ‡è®°ä¸º1ï¼‰ç»„æˆ\n",
    "# å‚æ•°: batch_index (int, optional, defaults to 0) â€” The index to access in the batch.\n",
    "# ä¸æŒ‡å®š batch_index æ—¶ï¼Œé»˜è®¤è¿”å›ç¬¬ä¸€ä¸ª batch çš„ sequence_ids\n",
    "sequence_ids = inputs.sequence_ids()\n",
    "print(sequence_ids)\n",
    "print(len(sequence_ids))\n",
    "\n",
    "outputs = model(**inputs)\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(start_logits.shape, end_logits.shape)\n",
    "\n",
    "# å±è”½ context tokens ä¹‹å¤–çš„æ‰€æœ‰å†…å®¹\n",
    "mask = [ i != 1 for i in sequence_ids]\n",
    "# å–æ¶ˆå¯¹ [CLS] token çš„å±è”½\n",
    "mask[0] = False\n",
    "# å±è”½æ‰€æœ‰çš„ [PAD] tokens\n",
    "mask = torch.logical_or(torch.tensor(mask)[None], (inputs[\"attention_mask\"] == 0))\n",
    "\n",
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000\n",
    "\n",
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)\n",
    "print(start_probabilities.shape, end_probabilities.shape)\n",
    "\n",
    "candidates = []\n",
    "for start_probs, end_probs in zip(start_probabilities, end_probabilities):\n",
    "    scores = start_probs[:, None] * end_probs[None, :]\n",
    "    idx = torch.triu(scores).argmax().item()\n",
    "\n",
    "    start_idx = idx // scores.shape[1]\n",
    "    end_idx = idx % scores.shape[1]\n",
    "    score = scores[start_idx, end_idx].item()\n",
    "    candidates.append((start_idx, end_idx, score))\n",
    "\n",
    "print(candidates)\n",
    "\n",
    "for candidate, offset in zip(candidates, offsets):\n",
    "    start_token, end_token, score = candidate\n",
    "    start_char, _ = offset[start_token]\n",
    "    _, end_char = offset[end_token]\n",
    "    answer = long_context[start_char:end_char]\n",
    "    result = {\"answer\": answer, \"start\": start_char, \"end\": end_char, \"score\": score}\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ ‡å‡†åŒ–å’Œé¢„åˆ†è¯\n",
    "tokenization çš„æ­¥éª¤:\n",
    "1. æ ‡å‡†åŒ–ï¼ˆä»»ä½•è®¤ä¸ºå¿…è¦çš„æ–‡æœ¬æ¸…ç†ï¼Œä¾‹å¦‚åˆ é™¤ç©ºæ ¼æˆ–é‡éŸ³ç¬¦å·ã€Unicode è§„èŒƒåŒ–ç­‰ï¼‰\n",
    "2. é¢„åˆ†è¯ï¼ˆå°†è¾“å…¥æ‹†åˆ†ä¸ºå•è¯ï¼‰\n",
    "3. é€šè¿‡æ¨¡å‹å¤„ç†è¾“å…¥ï¼ˆä½¿ç”¨é¢„å…ˆæ‹†åˆ†çš„è¯æ¥ç”Ÿæˆä¸€ç³»åˆ— tokens ï¼‰\n",
    "4. åå¤„ç†ï¼ˆæ·»åŠ  tokenizer çš„ç‰¹æ®Š tokens ç”Ÿæˆæ³¨æ„åŠ›æ©ç å’Œ token ç±»å‹ IDï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# BERT tokenizer åŸºäºWordPiece\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(type(tokenizer.backend_tokenizer))\n",
    "# é€šè¿‡ normalize_str æ–¹æ³•æ¥è§„èŒƒåŒ–å­—ç¬¦ä¸²\n",
    "print(tokenizer.backend_tokenizer.normalizer.normalize_str(\"HÃ©llÃ² hÃ´w are Ã¼?\"))\n",
    "\n",
    "# pre_tokenize_str() æ–¹æ³•å°†å­—ç¬¦ä¸²æ‹†åˆ†ä¸ºå•è¯\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸åŒçš„åˆ†è¯å™¨ï¼Œåˆ†è¯è§„åˆ™å­˜åœ¨å·®å¼‚ï¼Œæ¯”å¦‚ä¸Šé¢ BERT tokenizer ä¼šå°†ä¸¤ä¸ªç©ºæ ¼çœ‹åšä¸€ä¸ªç©ºæ ¼ï¼Œ\n",
    "# è€Œ GPT-2 tokenizer ä¸ä¼š\n",
    "# GPT-2 tokenizer åŸºäº Byte-Pair Encoding (BPE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸºäº SentencePiece ç®—æ³•çš„ T5 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE tokenization ç®—æ³•\n",
    "BPEï¼ˆByte Pair Encodingï¼Œå­—èŠ‚å¯¹ç¼–ç ï¼‰ æ˜¯ä¸€ç§åŸºäºç»Ÿè®¡çš„å­è¯åˆ†è¯ç®—æ³•ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡é€æ­¥åˆå¹¶é«˜é¢‘å­—ç¬¦å¯¹ï¼Œå°†æ–‡æœ¬æ‹†åˆ†ä¸ºæ›´å°çš„å­è¯å•å…ƒï¼ˆsubword unitsï¼‰ï¼Œä»è€Œå¹³è¡¡è¯æ±‡è¡¨å¤§å°ä¸æ¨¡å‹å¯¹ç½•è§è¯çš„å¤„ç†èƒ½åŠ›ã€‚\n",
    "1. **ä»å­—ç¬¦åˆ°å­è¯**ï¼šBPE ä»å­—ç¬¦çº§åˆ«å¼€å§‹ï¼Œé€æ­¥åˆå¹¶å‡ºç°é¢‘ç‡æœ€é«˜çš„ç›¸é‚»å­—ç¬¦å¯¹ï¼Œå½¢æˆæ›´é•¿çš„å­è¯å•å…ƒï¼Œç›´åˆ°è¾¾åˆ°é¢„è®¾çš„è¯æ±‡è¡¨å¤§å°ã€‚\n",
    "2. **é«˜é¢‘ä¼˜å…ˆ**ï¼šé«˜é¢‘çš„å­—ç¬¦ç»„åˆï¼ˆå¦‚è‹±æ–‡ä¸­çš„ \"ing\"ã€\"ed\"ï¼Œä¸­æ–‡ä¸­çš„å¸¸è§å­—å¯¹ï¼‰ä¼šè¢«ä¼˜å…ˆåˆå¹¶ï¼Œå½¢æˆæœ‰è¯­ä¹‰æ„ä¹‰çš„å­è¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰‹åŠ¨å®ç° BPE ç®—æ³•\n",
    "# 1. æä¾›ä¸€ä¸ªç®€å•çš„è¯­æ–™åº“\n",
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]\n",
    "\n",
    "# 2. åŠ è½½ gpt2 åˆ†è¯å™¨\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 3. ä½¿ç”¨gpt2åˆ†è¯å™¨è¿›è¡Œé¢„åˆ†è¯ï¼Œå¹¶è®¡ç®—è¯­æ–™åº“ä¸­æ¯ä¸ªå•è¯çš„é¢‘ç‡\n",
    "from collections import defaultdict\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "print(word_freqs)\n",
    "\n",
    "# 4. äº§å‡ºåŸºç¡€è¯æ±‡è¡¨ï¼ˆå­—ç¬¦ä¸²->å­—ç¬¦ï¼‰\n",
    "alphabet = []\n",
    "for word in word_freqs.keys():\n",
    "    for letter in word:\n",
    "        if letter not in alphabet:\n",
    "            alphabet.append(letter)\n",
    "alphabet.sort()\n",
    "print(alphabet)\n",
    "\n",
    "# 5. è¯æ±‡è¡¨å¼€å¤´å¢åŠ ä¸€ä¸ªç‰¹æ®Šæ ‡è¯†\n",
    "vocab = [\"<|endoftext|>\"] + alphabet.copy()\n",
    "print(vocab)\n",
    "\n",
    "# 6. å°†æ¯ä¸ªå•è¯æ‹†åˆ†æˆå•ç‹¬çš„å­—ç¬¦\n",
    "splits = {word: [c for c in word] for word in word_freqs.keys()}\n",
    "print(splits)\n",
    "\n",
    "# 7. æ¶‰åŠä¸€ä¸ª ç»Ÿè®¡æ¯å¯¹å­—ç¬¦çš„é¢‘ç‡ çš„å‡½æ•°\n",
    "def compute_pair_freqs(splits):\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            # å•ä¸ªå­—ç¬¦ä¸éœ€è¦åˆå¹¶\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            pair_freqs[pair] += freq\n",
    "    return pair_freqs\n",
    "\n",
    "# 8. åˆå¹¶å­—ç¬¦å¯¹ï¼Œå¹¶åŠ å…¥è¯æ±‡è¡¨\n",
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                split = split[:i] + [a + b] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits\n",
    "\n",
    "\"\"\"\n",
    "å°† 7ã€8 æ”¾åˆ°å¾ªç¯å†…ï¼Œä¾¿å¯ä»¥è·å–åˆ°æ‰€æœ‰æƒ³è¦çš„åˆå¹¶äº†\n",
    "\"\"\"\n",
    "# è®¾å®šè¯æ±‡è¡¨çš„å¤§å°\n",
    "vocab_size = 50\n",
    "merges = dict()\n",
    "while len(vocab) < vocab_size:\n",
    "    # ç”Ÿæˆ å­—ç¬¦å¯¹ å’Œ é¢‘ç‡\n",
    "    pair_freqs = compute_pair_freqs(splits)\n",
    "    # æ‰¾å‡ºé¢‘ç‡æœ€é«˜çš„å­—ç¬¦å¯¹\n",
    "    best_pair = \"\"\n",
    "    max_freq = None\n",
    "    for pair, freq in pair_freqs.items():\n",
    "        if max_freq is None or max_freq < freq:\n",
    "            best_pair = pair\n",
    "            max_freq = freq\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "    vocab.append(best_pair[0] + best_pair[1])\n",
    "print(merges)\n",
    "print(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•ä¸Šé¢ç”Ÿæˆçš„åˆ†è¯å™¨\n",
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
    "    for pair, merge in merges.items():\n",
    "        for idx, split in enumerate(splits):\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[idx] = split\n",
    "    return sum(splits, [])\n",
    "\n",
    "tokenize(\"This is not a token.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordPiece tokenization ç®—æ³•\n",
    "WordPiece æ˜¯å¦ä¸€ç§åŸºäºå­è¯çš„åˆ†è¯ç®—æ³•ï¼Œç”± Google æå‡ºå¹¶å¹¿æ³›åº”ç”¨äº BERT ç­‰é¢„è®­ç»ƒæ¨¡å‹ã€‚ä¸ BPEï¼ˆByte Pair Encodingï¼‰ç±»ä¼¼ï¼ŒWordPiece é€šè¿‡å°†æ–‡æœ¬åˆ†è§£ä¸ºæ›´å°çš„å­è¯å•å…ƒæ¥è§£å†³æœªç™»å½•è¯ï¼ˆOOVï¼‰é—®é¢˜ï¼Œä½†å®ƒåœ¨åˆå¹¶ç­–ç•¥å’Œè®­ç»ƒç›®æ ‡ä¸Šä¸ BPE æœ‰æ˜¾è‘—å·®å¼‚ã€‚\n",
    "- æ ¸å¿ƒæ€æƒ³ï¼šé€šè¿‡æœ€å¤§åŒ–è¯­è¨€æ¨¡å‹çš„ä¼¼ç„¶æ¦‚ç‡ï¼Œé€‰æ‹©åˆå¹¶åèƒ½æœ€å¤§ç¨‹åº¦æå‡å¥å­æ¦‚ç‡çš„å­—ç¬¦å¯¹ã€‚ï¼ˆä¸ BPE çš„â€œé«˜é¢‘ä¼˜å…ˆâ€åˆå¹¶ä¸åŒï¼ŒWordPiece çš„åˆå¹¶ç­–ç•¥æ›´å…³æ³¨å­è¯ç»„åˆå¯¹æ•´ä½“è¯­ä¹‰çš„è´¡çŒ®ã€‚ï¼‰\n",
    "\n",
    "è®¡ç®—åˆå¹¶å€™é€‰å¯¹çš„å¾—åˆ†å…¬å¼ï¼š    \n",
    "$$\n",
    "score=(freq\\_of\\_pair)/(freq\\_of\\_first\\_element \\times freq\\_of\\_second\\_element)\n",
    "$$\n",
    "é€šè¿‡å°†ä¸¤éƒ¨åˆ†åˆåœ¨ä¸€èµ·çš„é¢‘ç‡é™¤ä»¥å…¶ä¸­å„éƒ¨åˆ†çš„é¢‘ç‡çš„ä¹˜ç§¯ï¼Œè¯¥ç®—æ³•ä¼˜å…ˆåˆå¹¶é‚£äº›åœ¨è¯æ±‡è¡¨ä¸­å•ç‹¬å‡ºç°é¢‘ç‡è¾ƒä½çš„å¯¹ã€‚å¾—åˆ†è¶Šé«˜ï¼Œè¯´æ˜åˆå¹¶åå¯¹æ¨¡å‹è¶Šæœ‰åˆ©ã€‚  \n",
    "      \n",
    "æ³¨ï¼šWordPiece å’Œ BPE çš„åˆ†è¯æ–¹å¼æœ‰æ‰€ä¸åŒï¼ŒWordPiece åªä¿å­˜æœ€ç»ˆè¯æ±‡è¡¨ï¼Œè€Œä¸ä¿å­˜å­¦ä¹ åˆ°çš„åˆå¹¶è§„åˆ™ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰‹åŠ¨å®ç° WordPiece ç®—æ³•\n",
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]\n",
    "\n",
    "# åŠ è½½ç”¨äºé¢„åˆ†è¯çš„åˆ†è¯å™¨\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# ç»Ÿè®¡è¯­æ–™åº“ä¸­æ¯ä¸ªå•è¯å‡ºç°çš„é¢‘ç‡\n",
    "from collections import defaultdict\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "print(word_freqs)\n",
    "\n",
    "# ç”Ÿæˆå­—æ¯è¡¨ï¼Œæ³¨æ„é™¤äº†å•è¯çš„ç¬¬ä¸€ä¸ªå­—æ¯å¤–ï¼Œå…¶ä»–å­—ç¬¦å‰é¢éƒ½å¾—åŠ ä¸Šå‰ç¼€â€œ##â€\n",
    "alphabet = []\n",
    "for word in word_freqs.keys():\n",
    "    if word[0] not in alphabet:\n",
    "        alphabet.append(word[0])\n",
    "    for letter in word[1:]:\n",
    "        if f\"##{letter}\" not in alphabet:\n",
    "            alphabet.append(f\"##{letter}\")\n",
    "alphabet.sort()\n",
    "print(alphabet)\n",
    "\n",
    "# åœ¨è¯æ±‡è¡¨çš„å¼€å¤´æ·»åŠ äº†ç‰¹æ®Š tokens\n",
    "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()\n",
    "print(vocab)\n",
    "\n",
    "# å°†æ¯ä¸ªå•è¯è¿›è¡Œåˆ†å‰²ï¼Œé™¤äº†ç¬¬ä¸€ä¸ªå­—æ¯å¤–ï¼Œå…¶ä»–å­—æ¯éƒ½éœ€è¦ä»¥ \"##\" ä¸ºå‰ç¼€\n",
    "splits = {\n",
    "    word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
    "    for word in word_freqs.keys()\n",
    "}\n",
    "print(splits)\n",
    "\n",
    "# è®¡ç®—æ¯å¯¹çš„åˆ†æ•°\n",
    "def compute_pair_scores(splits):\n",
    "    letter_freqs = defaultdict(int)\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            letter_freqs[split[0]] += freq\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            letter_freqs[split[i]] += freq\n",
    "            pair_freqs[pair] += freq\n",
    "        letter_freqs[split[-1]] += freq\n",
    "\n",
    "    scores = {\n",
    "        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
    "        for pair, freq in pair_freqs.items()\n",
    "    }\n",
    "    return scores\n",
    "\n",
    "# å¯¹ splits å­—å…¸è¿›è¡Œåˆå¹¶\n",
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
    "                split = split[:i] + [merge] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits\n",
    "\n",
    "# å®šè¯æ±‡è¡¨çš„å¤§å°ä¸º 70\n",
    "vocab_size = 70\n",
    "while len(vocab) < vocab_size:\n",
    "    scores = compute_pair_scores(splits)\n",
    "    best_pair, max_score = \"\", None\n",
    "    for pair, score in scores.items():\n",
    "        if max_score is None or max_score < score:\n",
    "            best_pair = pair\n",
    "            max_score = score\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    new_token = (\n",
    "        best_pair[0] + best_pair[1][2:]\n",
    "        if best_pair[1].startswith(\"##\")\n",
    "        else best_pair[0] + best_pair[1]\n",
    "    )\n",
    "    vocab.append(new_token)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¦å¯¹æ–°æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œæˆ‘ä»¬å…ˆé¢„åˆ†è¯ï¼Œå†è¿›è¡Œåˆ†å‰²ï¼Œç„¶ååœ¨æ¯ä¸ªè¯ä¸Šä½¿ç”¨åˆ†è¯ç®—æ³•ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œ\n",
    "# æˆ‘ä»¬å¯»æ‰¾ä»ç¬¬ä¸€ä¸ªè¯å¼€å§‹çš„æœ€å¤§å­è¯å¹¶å°†å…¶åˆ†å‰²ï¼Œç„¶åæˆ‘ä»¬å¯¹ç¬¬äºŒéƒ¨åˆ†é‡å¤æ­¤è¿‡ç¨‹ï¼Œä»¥æ­¤ç±»æ¨ï¼Œ\n",
    "# å¯¹è¯¥è¯ä»¥åŠæ–‡æœ¬ä¸­çš„åç»­è¯è¿›è¡Œåˆ†å‰²\n",
    "def encode_word(word):\n",
    "    \"\"\"\n",
    "    å¯¹è¾“å…¥çš„å•è¯è¿›è¡Œåˆ†è¯\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    while len(word) > 0:\n",
    "        i = len(word)\n",
    "        while i > 0 and word[:i] not in vocab:\n",
    "            i -= 1\n",
    "        if i == 0:\n",
    "            return [\"[UNK]\"]\n",
    "        tokens.append(word[:i])\n",
    "        word = word[i:]\n",
    "        if len(word) > 0:\n",
    "            word = f\"##{word}\"\n",
    "    return tokens\n",
    "\n",
    "print(encode_word(\"Hugging\"))\n",
    "print(encode_word(\"HOgging\"))\n",
    "\n",
    "# å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯\n",
    "def tokenize(text):\n",
    "    # é¢„åˆ†è¯\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    encoded_words = [encode_word(word) for word in pre_tokenized_text]\n",
    "    return sum(encoded_words, [])\n",
    "print(tokenize(\"This is the Hugging Face course!\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram tokenization ç®—æ³•\n",
    "Unigram åˆ†è¯ç®—æ³• æ˜¯ä¸€ç§åŸºäºæ¦‚ç‡æ¨¡å‹çš„å­è¯åˆ†è¯æ–¹æ³•ï¼Œä¸ BPE å’Œ WordPiece ä¸åŒï¼Œå®ƒé€šè¿‡**æœ€å¤§åŒ–å¥å­æ¦‚ç‡**æ¥ç¡®å®šæœ€ä¼˜åˆ†è¯æ–¹å¼ã€‚Unigram å‡è®¾æ¯ä¸ªå­è¯çš„å‡ºç°æ˜¯ç‹¬ç«‹çš„ï¼Œå¹¶é€šè¿‡è¿­ä»£ä¼˜åŒ–è¯æ±‡è¡¨å’Œå­è¯æ¦‚ç‡ï¼Œé€‚ç”¨äºçµæ´»å¤„ç†å¤šè¯­è¨€å’Œå¤æ‚æ–‡æœ¬åœºæ™¯ã€‚   \n",
    "- æ¦‚ç‡é©±åŠ¨ï¼šæ¯ä¸ªå­è¯æœ‰ä¸€ä¸ªæ¦‚ç‡å€¼ï¼Œå¥å­çš„åˆ†è¯æ¦‚ç‡æ˜¯å…¶æ‰€æœ‰å­è¯æ¦‚ç‡çš„ä¹˜ç§¯ã€‚Unigram çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä½¿å¥å­æ¦‚ç‡æœ€å¤§çš„åˆ†è¯æ–¹å¼ã€‚\n",
    "- åŠ¨æ€è¯æ±‡è¡¨ï¼šä»ä¸€ä¸ªå¤§è¯æ±‡è¡¨å¼€å§‹ï¼Œé€æ­¥åˆ é™¤å¯¹æ•´ä½“ä¼¼ç„¶è´¡çŒ®æœ€å°çš„å­è¯ï¼Œæœ€ç»ˆå¾—åˆ°ç›®æ ‡å¤§å°çš„è¯æ±‡è¡¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰‹åŠ¨å®ç° Unigram tokenization ç®—æ³•\n",
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "\n",
    "# ç»Ÿè®¡å•è¯çš„é¢‘ç‡\n",
    "from collections import defaultdict\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "print(word_freqs)\n",
    "\n",
    "# ç»Ÿè®¡å­—ç¬¦çš„é¢‘ç‡\n",
    "char_freqs = defaultdict(int)\n",
    "# ç”Ÿæˆæ¯ä¸ªå•è¯çš„å…¨éƒ¨å­ä¸²ï¼Œå¹¶ç»Ÿè®¡å­ä¸²çš„é¢‘ç‡\n",
    "subwords_freqs = defaultdict(int)\n",
    "for word, freq in word_freqs.items():\n",
    "    for i in range(len(word)):\n",
    "        char_freqs[word[i]] += freq\n",
    "        # å¾ªç¯éå†é•¿åº¦è‡³å°‘ä¸º2çš„å­å­—\n",
    "        for j in range(i + 2, len(word) + 1):\n",
    "            subwords_freqs[word[i:j]] += freq\n",
    "# æŒ‰é¢‘ç‡å¯¹å­è¯æ’åº\n",
    "sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "print(char_freqs)\n",
    "print(sorted_subwords[:20])\n",
    "\n",
    "# å°†é¢‘ç‡é å‰çš„å­è¯ å’Œ å•å­—ç¬¦ èåˆåœ¨ä¸€èµ·ï¼Œå¾—åˆ°ä¸€ä¸ªå¤§å°ä¸º300çš„åˆå§‹è¯æ±‡è¡¨\n",
    "token_freqs = list(char_freqs.items()) + sorted_subwords[: 300 - len(char_freqs)]\n",
    "token_freqs = {token: freq for token, freq in token_freqs}\n",
    "print(list(token_freqs.items())[:10])\n",
    "\n",
    "# è®¡ç®—æ‰€æœ‰é¢‘ç‡çš„æ€»å’Œï¼Œå°†é¢‘ç‡è½¬åŒ–ä¸ºæ¦‚ç‡\n",
    "# ç›¸è¾ƒäºå°æ•°ç›¸ä¹˜ï¼Œå¯¹æ•°ç›¸åŠ åœ¨æ•°å€¼ä¸Šæ›´ç¨³å®šï¼Œè€Œä¸”è¿™å°†ç®€åŒ–æ¨¡å‹æŸå¤±çš„è®¡ç®—\n",
    "from math import log\n",
    "total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "print(total_sum)\n",
    "model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}\n",
    "print(list(model.items())[:10])\n",
    "\n",
    "def encode_word(word, model):\n",
    "    # ä¸ºè¯çš„æ¯ä¸€ä¸ªä½ç½®ï¼ˆä» 0 å¼€å§‹ï¼Œä¸€ç›´åˆ°è¯çš„æ€»é•¿åº¦ï¼‰éƒ½ä¿å­˜ä¸€ä¸ªå­—å…¸ï¼Œ\n",
    "    # è®°å½•æ¯ä¸ªä½ç½®çš„æœ€ä½³åˆ†è¯èµ·ç‚¹å’Œç´¯è®¡å¾—åˆ†ã€‚\n",
    "    best_segmentations = [{\"start\": 0, \"score\": 1}] + [\n",
    "        {\"start\": None, \"score\": None} for _ in range(len(word))\n",
    "    ]\n",
    "    # å¡«å……åŠ¨æ€è§„åˆ’è¡¨\n",
    "    for start_idx in range(len(word)):\n",
    "        # éå†æ‰€æœ‰å¯èƒ½çš„èµ·å§‹ä½ç½® start_idx \n",
    "        # best_score_at_startåº”è¯¥ç”±å¾ªç¯çš„å‰é¢çš„æ­¥éª¤è®¡ç®—å’Œå¡«å……\n",
    "        best_score_at_start = best_segmentations[start_idx][\"score\"]\n",
    "        for end_idx in range(start_idx + 1, len(word) + 1):\n",
    "            # éå†æ‰€æœ‰å¯èƒ½çš„ç»“æŸä½ç½® end_idxï¼Œç”Ÿæˆå­è¯ tokenã€‚\n",
    "            token = word[start_idx:end_idx]\n",
    "            if token in model and best_score_at_start is not None:\n",
    "                # å¦‚æœå­è¯ token å­˜åœ¨äº model ä¸­ï¼Œåˆ™è®¡ç®—å½“å‰åˆ†æ®µçš„ç´¯è®¡å¾—\n",
    "                # åˆ†ï¼ˆmodel[token] + å½“å‰èµ·ç‚¹çš„å¾—åˆ†ï¼‰ã€‚\n",
    "                score = model[token] + best_score_at_start\n",
    "                # å¦‚æœæˆ‘ä»¬å‘ç°ä»¥ end_idx ç»“å°¾çš„æ›´å¥½åˆ†æ®µ,æˆ‘ä»¬ä¼šæ›´æ–°\n",
    "                # score æ˜¯ token çš„æ¦‚ç‡çš„è´Ÿå¯¹æ•°ï¼ˆ-log(probability)ï¼‰ï¼Œå› æ­¤åˆ†æ•°è¶Šå°è¡¨ç¤ºæ¦‚ç‡è¶Šå¤§ã€‚\n",
    "                if (\n",
    "                    best_segmentations[end_idx][\"score\"] is None\n",
    "                    or best_segmentations[end_idx][\"score\"] > score\n",
    "                ):\n",
    "                    # ä¿ç•™å¾—åˆ†æ›´é«˜çš„åˆ†è¯è·¯å¾„\n",
    "                    best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score}\n",
    "    # å›æº¯æœ€ä¼˜è·¯å¾„\n",
    "    segmentation = best_segmentations[-1]\n",
    "    if segmentation[\"score\"] is None:\n",
    "        # æˆ‘ä»¬æ²¡æœ‰æ‰¾åˆ°å•è¯çš„ tokens  -> unknown\n",
    "        return [\"<unk>\"], None\n",
    "    score = segmentation[\"score\"]\n",
    "    start = segmentation[\"start\"]\n",
    "    # ä»å•è¯æœ«å°¾ï¼ˆlen(word)ï¼‰å¼€å§‹ï¼Œé€šè¿‡ start å­—æ®µé€æ­¥å›æº¯åˆ°å¼€å¤´ï¼Œæå–æ‰€æœ‰å­è¯\n",
    "    end = len(word)\n",
    "    tokens = []\n",
    "    while start != 0:\n",
    "        # å°†å­è¯æŒ‰é¡ºåºæ’å…¥å•è¯è¡¨å¤´éƒ¨ï¼Œç¡®ä¿æœ€ç»ˆåˆ†è¯é¡ºåºæ­£ç¡®\n",
    "        tokens.insert(0, word[start:end])\n",
    "        next_start = best_segmentations[start][\"start\"]\n",
    "        end = start\n",
    "        start = next_start\n",
    "    tokens.insert(0, word[start:end])\n",
    "    return tokens, score\n",
    "\n",
    "print(encode_word(\"Hopefully\", model))\n",
    "print(encode_word(\"This\", model))\n",
    "\n",
    "def compute_loss(model):\n",
    "    loss = 0\n",
    "    for word, freq in word_freqs.items():\n",
    "        _, word_loss = encode_word(word, model)\n",
    "        loss += freq * word_loss\n",
    "    return loss\n",
    "\n",
    "import copy\n",
    "def compute_scores(model):\n",
    "    scores = {}\n",
    "    model_loss = compute_loss(model)\n",
    "    for token, score in model.items():\n",
    "        # æˆ‘ä»¬å°†ä¿ç•™é•¿åº¦ä¸º 1 çš„ tokens\n",
    "        if len(token) == 1:\n",
    "            continue\n",
    "        model_without_token = copy.deepcopy(model)\n",
    "        _ = model_without_token.pop(token)\n",
    "        scores[token] = compute_loss(model_without_token) - model_loss\n",
    "    return scores\n",
    "\n",
    "# è®¾ç½®è¯æ±‡è¡¨æœ€ç»ˆå¤§å°ä¸º100\n",
    "percent_to_remove = 0.1\n",
    "while len(model) > 100:\n",
    "    scores = compute_scores(model)\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1])\n",
    "    # åˆ é™¤åˆ†æ•°æœ€ä½çš„percent_to_remov tokens\n",
    "    for i in range(int(len(model) * percent_to_remove)):\n",
    "        _ = token_freqs.pop(sorted_scores[i][0])\n",
    "    total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}\n",
    "\n",
    "# å¯¹æ–°æ–‡æœ¬è¿›è¡Œ tokenization\n",
    "def tokenize(text, model):\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in words_with_offsets]\n",
    "    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]\n",
    "    return sum(encoded_words, [])\n",
    "\n",
    "\n",
    "print(tokenize(\"This is the Hugging Face course.\", model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¨¡å—åŒ–æ„å»º tokenizer\n",
    "`Tokenizers`åº“æ—¨åœ¨ä¸º tokenization æ¯ä¸ªæ­¥éª¤æä¾›å¤šä¸ªé€‰é¡¹ï¼Œä½ å¯ä»¥ä»»æ„æ­é…è¿™äº›é€‰é¡¹ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬å°†ä¼šä»é›¶å¼€å§‹æ„å»º`tokenizer`ï¼ˆä¸æ˜¯åŸºäºæ—§çš„`tokenizer`è®­ç»ƒï¼‰ï¼Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è·å–è¯­æ–™åº“\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")\n",
    "def get_training_corpus():\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i : i + 1000][\"text\"]\n",
    "\n",
    "# ç¦»çº¿ä¿å­˜åœ¨æœ¬åœ°\n",
    "with open(\"wikitext-2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(len(dataset)):\n",
    "        f.write(dataset[i][\"text\"] + \"\\n\")\n",
    "\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªä½¿ç”¨ WordPiece æ¨¡å‹çš„ Tokenizer \n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "# step1 æ ‡å‡†åŒ–\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
    ")\n",
    "print(tokenizer.normalizer.normalize_str(\"HÃ©llÃ² hÃ´w are Ã¼?\"))\n",
    "\n",
    "# step2 é¢„åˆ†è¯\n",
    "# Whitespace ä¼šä½¿ç”¨ç©ºæ ¼å’Œæ‰€æœ‰ä¸æ˜¯å­—æ¯ã€æ•°å­—æˆ–ä¸‹åˆ’çº¿çš„å­—ç¬¦è¿›è¡Œåˆ†å‰²\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "print(tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\"))\n",
    "# WhitespaceSplit åªä½¿ç”¨ç©ºæ ¼è¿›è¡Œåˆ†å‰²\n",
    "pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "print(pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\"))\n",
    "# å’Œ normalizer ä¸€æ ·ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ Sequence æ¥ç»„åˆå‡ ä¸ªé¢„åˆ†è¯çš„æ­¥éª¤\n",
    "pre_tokenizer = pre_tokenizers.Sequence(\n",
    "    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]\n",
    ")\n",
    "print(pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\"))\n",
    "\n",
    "# step3 é€šè¿‡æ¨¡å‹å¤„ç†è¾“å…¥\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n",
    "# # å¦‚æœåŸºäºæœ¬åœ°æ–‡ä»¶çš„è¯å¯ä»¥æŒ‰ä¸‹é¢çš„æ–¹å¼è®­ç»ƒæ¨¡å‹\n",
    "# tokenizer.model = models.WordPiece(unk_token=\"[UNK]\")\n",
    "# tokenizer.train([\"wikitext-2.txt\"], trainer=trainer)\n",
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)\n",
    "\n",
    "# step4 åå¤„ç†\n",
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "print(cls_token_id, sep_token_id)\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
    ")\n",
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)\n",
    "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences.\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)\n",
    "\n",
    "# step5 æŒ‡å®šä¸€ä¸ªè§£ç å™¨\n",
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")\n",
    "print(tokenizer.decode(encoding.ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
