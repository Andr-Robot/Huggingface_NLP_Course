{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Tokenizers库](https://huggingface.co/learn/nlp-course/zh-CN/chapter6)\n",
    "当我们需要微调模型时，我们需要使用与模型预训练相同的`tokenizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于已有的 tokenizer 训练新的 tokenizer\n",
    "大多数`Transformer 模型`使用`子词分词算法`。为了找到语料库中的常见子词，`tokenizer`需要深入统计语料库中的所有文本——这个过程我们称之为`训练（training）`。具体的训练规则取决于使用的`tokenizer`类型。\n",
    "> 训练 tokenizer 与训练模型不同！模型训练使用随机梯度下降使每个 batch 的 loss 小一点。它本质上是随机的（这意味着在即使两次训练的参数和算法完全相同，你也必须设置一些随机数种子才能获得相同的结果）。训练 tokenizer 是一个统计过程，它试图确定哪些子词最适合为给定的语料库选择，确定的过程取决于分词算法。它是确定性的，这意味着在相同的语料库上使用相同的算法进行训练时，得到的结果总是相同的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1 准备语料库\n",
    "from datasets import load_dataset\n",
    "\n",
    "# # 下载不下来数据的时候可以增加魔法\n",
    "# import os\n",
    "# os.environ[\"http_proxy\"] = \"http://127.0.0.1:port\"\n",
    "# os.environ[\"https_proxy\"] = \"http://127.0.0.1:port\"\n",
    "\n",
    "# # 加载数据集\n",
    "raw_datasets = load_dataset(\"code_search_net\", \"python\", trust_remote_code=True)\n",
    "print(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们仅使用 whole_func_string 列来训练我们的 tokenizer \n",
    "# 我们可以通过索引来查看其中一个函数的示例\n",
    "print(raw_datasets[\"train\"][123456][\"whole_func_string\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建迭代器\n",
    "# 将数据集转换为一个文本列表的iterator（迭代器），这样就可以以batch的方式进行训练了\n",
    "# 使用迭代器可以不把所有内容都加载到内存中\n",
    "# 通过 python 的 generator 来实现\n",
    "## version 1 基于列表的 generator\n",
    "def get_training_corpus():\n",
    "    return (\n",
    "        raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
    "        for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    "    )\n",
    "\n",
    "\n",
    "training_corpus = get_training_corpus()\n",
    "print(training_corpus)\n",
    "\n",
    "## version 2 基于 yield 的 generator\n",
    "def get_training_corpus_v2():\n",
    "    dataset = raw_datasets[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"whole_func_string\"]\n",
    "    \n",
    "training_corpus_v2 = get_training_corpus_v2()\n",
    "print(training_corpus_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step2 训练一个新的 tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "# 使用旧的 tokenizer，训练一个新的 tokenizer\n",
    "# 新的 tokenizer 将与 GPT-2 完全相同，唯一的区别是词汇表，这将由我们的语料库通过训练来重新确定。\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "example = '''def add_numbers(a, b):\n",
    "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "    return a + b'''\n",
    "\n",
    "tokens = old_tokenizer.tokenize(example)\n",
    "# 展示在旧的 tokenizer 下的 token\n",
    "print(tokens)\n",
    "# 使用code_search_net中的Python数据集，训练一个新的 tokenizer ，词汇表大小为52000\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)\n",
    "tokens = tokenizer.tokenize(example)\n",
    "print(tokens)\n",
    "print(len(tokens))\n",
    "print(len(old_tokenizer.tokenize(example)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token-classification\n",
    "`Token-classification`是自然语言处理（NLP）中的核心任务，旨在为文本中的每个 token（如单词、子词或字符）分配一个特定的标签。它的核心目标是通过对文本的细粒度分析，提取结构化信息或理解语言单元的语义角色，广泛应用于命名实体识别（NER）、分词、词性标注等场景。    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用pipeline，默认基于 dbmdz/bert-large-cased-finetuned-conll03-english 模型对句子进行 NER\n",
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\"token-classification\")\n",
    "print(token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\"))\n",
    "# 可以使用将同一实体的 token 组合在一起\n",
    "## aggregation_strategy 通过设置，可以设置组合后实体计算的策略\n",
    "token_classifier = pipeline(\"token-classification\", aggregation_strategy=\"simple\")\n",
    "print(token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自己构建pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "inputs = tokenizer(example, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "# 输入的token的维度\n",
    "print(inputs[\"input_ids\"].shape)\n",
    "# 输出的维度\n",
    "print(outputs.logits.shape)\n",
    "# 打印索引到标签的映射\n",
    "print(model.config.id2label)\n",
    "\n",
    "import torch\n",
    "# 使用 softmax 函数将这些 logits 转化为概率，并取 argmax 来得到预测\n",
    "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()\n",
    "predictions = outputs.logits.argmax(dim=-1)[0].tolist()\n",
    "print(predictions)\n",
    "\n",
    "# 格式化输出 token 的得分和标签\n",
    "results = []\n",
    "# 获取偏移映射\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        start, end = offsets[idx]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity\": label,\n",
    "                \"score\": probabilities[idx][pred],\n",
    "                \"word\": tokens[idx],\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(results)\n",
    "\n",
    "# 格式化输出组合后 token 的得分和标签\n",
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "idx = 0\n",
    "while idx < len(predictions):\n",
    "    pred = predictions[idx]\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        # 删除 B- 或者 I-\n",
    "        label = label[2:]\n",
    "        start, _ = offsets[idx]\n",
    "\n",
    "        # 获取所有标有 I 标签的token\n",
    "        all_scores = []\n",
    "        while (\n",
    "            idx < len(predictions)\n",
    "            and model.config.id2label[predictions[idx]] == f\"I-{label}\"\n",
    "        ):\n",
    "            all_scores.append(probabilities[idx][pred])\n",
    "            _, end = offsets[idx]\n",
    "            idx += 1\n",
    "\n",
    "        # 分数是该分组实体中所有token分数的平均值\n",
    "        score = np.mean(all_scores).item()\n",
    "        word = example[start:end]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity_group\": label,\n",
    "                \"score\": score,\n",
    "                \"word\": word,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "    idx += 1\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question-answering\n",
    "question-answering pipeline 用于执行问答任务。具体来说，它用于从给定的文本中提取答案，基于一个预训练的模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "context = \"\"\"\n",
    "🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch, and TensorFlow — with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "long_context = \"\"\"\n",
    "🤗 Transformers: State of the Art NLP\n",
    "\n",
    "🤗 Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\n",
    "question answering, summarization, translation, text generation and more in over 100 languages.\n",
    "Its aim is to make cutting-edge NLP easier to use for everyone.\n",
    "\n",
    "🤗 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\n",
    "then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\n",
    "can be modified to enable quick research experiments.\n",
    "\n",
    "Why should I use transformers?\n",
    "\n",
    "1. Easy-to-use state-of-the-art models:\n",
    "  - High performance on NLU and NLG tasks.\n",
    "  - Low barrier to entry for educators and practitioners.\n",
    "  - Few user-facing abstractions with just three classes to learn.\n",
    "  - A unified API for using all our pretrained models.\n",
    "  - Lower compute costs, smaller carbon footprint:\n",
    "\n",
    "2. Researchers can share trained models instead of always retraining.\n",
    "  - Practitioners can reduce compute time and production costs.\n",
    "  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n",
    "\n",
    "3. Choose the right framework for every part of a model's lifetime:\n",
    "  - Train state-of-the-art models in 3 lines of code.\n",
    "  - Move a single model between TF2.0/PyTorch frameworks at will.\n",
    "  - Seamlessly pick the right framework for training, evaluation and production.\n",
    "\n",
    "4. Easily customize a model or an example to your needs:\n",
    "  - We provide examples for each architecture to reproduce the results published by its original authors.\n",
    "  - Model internals are exposed as consistently as possible.\n",
    "  - Model files can be used independently of the library for quick experiments.\n",
    "\n",
    "🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "\n",
    "question = \"Which deep learning libraries back 🤗 Transformers?\"\n",
    "print(question_answerer(question=question, context=context))\n",
    "# 长文本也能处理\n",
    "print(question_answerer(question=question, context=long_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自己构建pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "model_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "# 输入格式是：[CLS] question [SEP] context [SEP] \n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "# 问答模型训练的目标是来预测答案开始的 token 的索引和答案结束的 token 的索引。\n",
    "# 所以返回值是两个logits：一个对应于答案的开始 token 的 logits，另一个对应于答案的结束 token 的 logits。\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(start_logits.shape, end_logits.shape)\n",
    "\n",
    "# 使用 softmax 函数将这些 logits 转化为概率，并取 argmax 来得到预测\n",
    "\"\"\"\n",
    "注意：\n",
    "由于输入格式是 [CLS] question [SEP] context [SEP] ，所以我们需要屏蔽 question 的 tokens 以及 [SEP] token 。\n",
    "不过，我们将保留 [CLS] ，因为某些模型使用它来表示答案不在上下文中。\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "sequence_ids = inputs.sequence_ids()\n",
    "# 屏蔽除 context 之外的所有内容\n",
    "mask = [i != 1 for i in sequence_ids]\n",
    "# 不屏蔽 [CLS] token\n",
    "mask[0] = False\n",
    "mask = torch.tensor(mask)[None]\n",
    "print(mask)\n",
    "# 将想要屏蔽的 logits 替换为一个大的负数\n",
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000\n",
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]\n",
    "print(start_probabilities.shape, end_probabilities.shape)\n",
    "\n",
    "# 在满足 start_index <= end_index 的前提下，计算每个可能的 start_index 和 end_index 的概率，\n",
    "# 然后取概率最高的 (start_index, end_index) 元组。\n",
    "# start_probabilities[start_index] × end_probabilities[end_index] 就是这个 (start_index, end_index) 元组的概率。\n",
    "scores = start_probabilities[:, None] * end_probabilities[None, :]\n",
    "print(scores.shape)\n",
    "import numpy as np\n",
    "# 获取上三角部分\n",
    "scores = torch.triu(scores)\n",
    "\n",
    "# argmax 将返回展平（flattened）后张量中的索引\n",
    "max_index = scores.argmax().item()\n",
    "start_index = max_index // scores.shape[1]\n",
    "end_index = max_index % scores.shape[1]\n",
    "print(start_index, end_index)\n",
    "print(scores[start_index, end_index])\n",
    "\n",
    "# 有了start_index 和 end_index，可以从 context 中提取答案\n",
    "inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "start_char, _ = offsets[start_index]\n",
    "_, end_char = offsets[end_index]\n",
    "answer = context[start_char:end_char]\n",
    "# 格式化输出\n",
    "result = {\n",
    "    \"answer\": answer,\n",
    "    \"start\": start_char,\n",
    "    \"end\": end_char,\n",
    "    \"score\": scores[start_index, end_index],\n",
    "}\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理长文本\n",
    "\"\"\"\n",
    "question-answering pipeline \n",
    "通过设置max_length，限制tokens数量的最大值为384\n",
    "当超过这个限制时，我们需要将长文本分割成多个小段，然后在小段中寻找答案\n",
    "为了防止答案被分割，我们需要在各块之间重叠一些内容\n",
    "\n",
    "通过使用以下参数实现：\n",
    "1. return_overflowing_tokens=True，当输入文本长度超过模型支持的最大长度时，\n",
    "   该参数会将文本分割成多个子片段（chunks），并返回所有溢出片段。\n",
    "2. stride 定义分割后的子片段之间的重叠token数量。\n",
    "\"\"\"\n",
    "inputs = tokenizer(\n",
    "    question,\n",
    "    long_context,\n",
    "    stride=128,\n",
    "    max_length=384,\n",
    "    padding=\"longest\",\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "_ = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "offsets = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "inputs = inputs.convert_to_tensors(\"pt\")\n",
    "print(inputs[\"input_ids\"].shape)\n",
    "# sequence_ids 用于标识输入中不同句子的边界（常见于多序列输入场景）。\n",
    "# 例如在问答任务中，输入由问题（标记为0）和上下文（标记为1）组成\n",
    "# 参数: batch_index (int, optional, defaults to 0) — The index to access in the batch.\n",
    "# 不指定 batch_index 时，默认返回第一个 batch 的 sequence_ids\n",
    "sequence_ids = inputs.sequence_ids()\n",
    "print(sequence_ids)\n",
    "print(len(sequence_ids))\n",
    "\n",
    "outputs = model(**inputs)\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(start_logits.shape, end_logits.shape)\n",
    "\n",
    "# 屏蔽 context tokens 之外的所有内容\n",
    "mask = [ i != 1 for i in sequence_ids]\n",
    "# 取消对 [CLS] token 的屏蔽\n",
    "mask[0] = False\n",
    "# 屏蔽所有的 [PAD] tokens\n",
    "mask = torch.logical_or(torch.tensor(mask)[None], (inputs[\"attention_mask\"] == 0))\n",
    "\n",
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000\n",
    "\n",
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)\n",
    "print(start_probabilities.shape, end_probabilities.shape)\n",
    "\n",
    "candidates = []\n",
    "for start_probs, end_probs in zip(start_probabilities, end_probabilities):\n",
    "    scores = start_probs[:, None] * end_probs[None, :]\n",
    "    idx = torch.triu(scores).argmax().item()\n",
    "\n",
    "    start_idx = idx // scores.shape[1]\n",
    "    end_idx = idx % scores.shape[1]\n",
    "    score = scores[start_idx, end_idx].item()\n",
    "    candidates.append((start_idx, end_idx, score))\n",
    "\n",
    "print(candidates)\n",
    "\n",
    "for candidate, offset in zip(candidates, offsets):\n",
    "    start_token, end_token, score = candidate\n",
    "    start_char, _ = offset[start_token]\n",
    "    _, end_char = offset[end_token]\n",
    "    answer = long_context[start_char:end_char]\n",
    "    result = {\"answer\": answer, \"start\": start_char, \"end\": end_char, \"score\": score}\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标准化和预分词\n",
    "tokenization 的步骤:\n",
    "1. 标准化（任何认为必要的文本清理，例如删除空格或重音符号、Unicode 规范化等）\n",
    "2. 预分词（将输入拆分为单词）\n",
    "3. 通过模型处理输入（使用预先拆分的词来生成一系列 tokens ）\n",
    "4. 后处理（添加 tokenizer 的特殊 tokens 生成注意力掩码和 token 类型 ID）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# BERT tokenizer 基于WordPiece\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(type(tokenizer.backend_tokenizer))\n",
    "# 通过 normalize_str 方法来规范化字符串\n",
    "print(tokenizer.backend_tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))\n",
    "\n",
    "# pre_tokenize_str() 方法将字符串拆分为单词\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不同的分词器，分词规则存在差异，比如上面 BERT tokenizer 会将两个空格看做一个空格，\n",
    "# 而 GPT-2 tokenizer 不会\n",
    "# GPT-2 tokenizer 基于 Byte-Pair Encoding (BPE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于 SentencePiece 算法的 T5 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE tokenization 算法\n",
    "BPE（Byte Pair Encoding，字节对编码） 是一种基于统计的子词分词算法。其核心思想是通过逐步合并高频字符对，将文本拆分为更小的子词单元（subword units），从而平衡词汇表大小与模型对罕见词的处理能力。\n",
    "1. **从字符到子词**：BPE 从字符级别开始，逐步合并出现频率最高的相邻字符对，形成更长的子词单元，直到达到预设的词汇表大小。\n",
    "2. **高频优先**：高频的字符组合（如英文中的 \"ing\"、\"ed\"，中文中的常见字对）会被优先合并，形成有语义意义的子词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手动实现 BPE 算法\n",
    "# 1. 提供一个简单的语料库\n",
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]\n",
    "\n",
    "# 2. 加载 gpt2 分词器\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 3. 使用gpt2分词器进行预分词，并计算语料库中每个单词的频率\n",
    "from collections import defaultdict\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "print(word_freqs)\n",
    "\n",
    "# 4. 产出基础词汇表（字符串->字符）\n",
    "alphabet = []\n",
    "for word in word_freqs.keys():\n",
    "    for letter in word:\n",
    "        if letter not in alphabet:\n",
    "            alphabet.append(letter)\n",
    "alphabet.sort()\n",
    "print(alphabet)\n",
    "\n",
    "# 5. 词汇表开头增加一个特殊标识\n",
    "vocab = [\"<|endoftext|>\"] + alphabet.copy()\n",
    "print(vocab)\n",
    "\n",
    "# 6. 将每个单词拆分成单独的字符\n",
    "splits = {word: [c for c in word] for word in word_freqs.keys()}\n",
    "print(splits)\n",
    "\n",
    "# 7. 涉及一个 统计每对字符的频率 的函数\n",
    "def compute_pair_freqs(splits):\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            # 单个字符不需要合并\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            pair_freqs[pair] += freq\n",
    "    return pair_freqs\n",
    "\n",
    "# 8. 合并字符对，并加入词汇表\n",
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                split = split[:i] + [a + b] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits\n",
    "\n",
    "\"\"\"\n",
    "将 7、8 放到循环内，便可以获取到所有想要的合并了\n",
    "\"\"\"\n",
    "# 设定词汇表的大小\n",
    "vocab_size = 50\n",
    "merges = dict()\n",
    "while len(vocab) < vocab_size:\n",
    "    # 生成 字符对 和 频率\n",
    "    pair_freqs = compute_pair_freqs(splits)\n",
    "    # 找出频率最高的字符对\n",
    "    best_pair = \"\"\n",
    "    max_freq = None\n",
    "    for pair, freq in pair_freqs.items():\n",
    "        if max_freq is None or max_freq < freq:\n",
    "            best_pair = pair\n",
    "            max_freq = freq\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "    vocab.append(best_pair[0] + best_pair[1])\n",
    "print(merges)\n",
    "print(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试上面生成的分词器\n",
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
    "    for pair, merge in merges.items():\n",
    "        for idx, split in enumerate(splits):\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[idx] = split\n",
    "    return sum(splits, [])\n",
    "\n",
    "tokenize(\"This is not a token.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordPiece tokenization 算法\n",
    "WordPiece 是另一种基于子词的分词算法，由 Google 提出并广泛应用于 BERT 等预训练模型。与 BPE（Byte Pair Encoding）类似，WordPiece 通过将文本分解为更小的子词单元来解决未登录词（OOV）问题，但它在合并策略和训练目标上与 BPE 有显著差异。\n",
    "- 核心思想：通过最大化语言模型的似然概率，选择合并后能最大程度提升句子概率的字符对。（与 BPE 的“高频优先”合并不同，WordPiece 的合并策略更关注子词组合对整体语义的贡献。）\n",
    "\n",
    "计算合并候选对的得分公式：    \n",
    "$$\n",
    "score=(freq\\_of\\_pair)/(freq\\_of\\_first\\_element \\times freq\\_of\\_second\\_element)\n",
    "$$\n",
    "通过将两部分合在一起的频率除以其中各部分的频率的乘积，该算法优先合并那些在词汇表中单独出现频率较低的对。得分越高，说明合并后对模型越有利。  \n",
    "      \n",
    "注：WordPiece 和 BPE 的分词方式有所不同，WordPiece 只保存最终词汇表，而不保存学习到的合并规则。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手动实现 WordPiece 算法\n",
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]\n",
    "\n",
    "# 加载用于预分词的分词器\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# 统计语料库中每个单词出现的频率\n",
    "from collections import defaultdict\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "print(word_freqs)\n",
    "\n",
    "# 生成字母表，注意除了单词的第一个字母外，其他字符前面都得加上前缀“##”\n",
    "alphabet = []\n",
    "for word in word_freqs.keys():\n",
    "    if word[0] not in alphabet:\n",
    "        alphabet.append(word[0])\n",
    "    for letter in word[1:]:\n",
    "        if f\"##{letter}\" not in alphabet:\n",
    "            alphabet.append(f\"##{letter}\")\n",
    "alphabet.sort()\n",
    "print(alphabet)\n",
    "\n",
    "# 在词汇表的开头添加了特殊 tokens\n",
    "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()\n",
    "print(vocab)\n",
    "\n",
    "# 将每个单词进行分割，除了第一个字母外，其他字母都需要以 \"##\" 为前缀\n",
    "splits = {\n",
    "    word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
    "    for word in word_freqs.keys()\n",
    "}\n",
    "print(splits)\n",
    "\n",
    "# 计算每对的分数\n",
    "def compute_pair_scores(splits):\n",
    "    letter_freqs = defaultdict(int)\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            letter_freqs[split[0]] += freq\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            letter_freqs[split[i]] += freq\n",
    "            pair_freqs[pair] += freq\n",
    "        letter_freqs[split[-1]] += freq\n",
    "\n",
    "    scores = {\n",
    "        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
    "        for pair, freq in pair_freqs.items()\n",
    "    }\n",
    "    return scores\n",
    "\n",
    "# 对 splits 字典进行合并\n",
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
    "                split = split[:i] + [merge] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits\n",
    "\n",
    "# 定词汇表的大小为 70\n",
    "vocab_size = 70\n",
    "while len(vocab) < vocab_size:\n",
    "    scores = compute_pair_scores(splits)\n",
    "    best_pair, max_score = \"\", None\n",
    "    for pair, score in scores.items():\n",
    "        if max_score is None or max_score < score:\n",
    "            best_pair = pair\n",
    "            max_score = score\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    new_token = (\n",
    "        best_pair[0] + best_pair[1][2:]\n",
    "        if best_pair[1].startswith(\"##\")\n",
    "        else best_pair[0] + best_pair[1]\n",
    "    )\n",
    "    vocab.append(new_token)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 要对新文本进行分词，我们先预分词，再进行分割，然后在每个词上使用分词算法。也就是说，\n",
    "# 我们寻找从第一个词开始的最大子词并将其分割，然后我们对第二部分重复此过程，以此类推，\n",
    "# 对该词以及文本中的后续词进行分割\n",
    "def encode_word(word):\n",
    "    \"\"\"\n",
    "    对输入的单词进行分词\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    while len(word) > 0:\n",
    "        i = len(word)\n",
    "        while i > 0 and word[:i] not in vocab:\n",
    "            i -= 1\n",
    "        if i == 0:\n",
    "            return [\"[UNK]\"]\n",
    "        tokens.append(word[:i])\n",
    "        word = word[i:]\n",
    "        if len(word) > 0:\n",
    "            word = f\"##{word}\"\n",
    "    return tokens\n",
    "\n",
    "print(encode_word(\"Hugging\"))\n",
    "print(encode_word(\"HOgging\"))\n",
    "\n",
    "# 对文本进行分词\n",
    "def tokenize(text):\n",
    "    # 预分词\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    encoded_words = [encode_word(word) for word in pre_tokenized_text]\n",
    "    return sum(encoded_words, [])\n",
    "print(tokenize(\"This is the Hugging Face course!\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram tokenization 算法\n",
    "Unigram 分词算法 是一种基于概率模型的子词分词方法，与 BPE 和 WordPiece 不同，它通过**最大化句子概率**来确定最优分词方式。Unigram 假设每个子词的出现是独立的，并通过迭代优化词汇表和子词概率，适用于灵活处理多语言和复杂文本场景。   \n",
    "- 概率驱动：每个子词有一个概率值，句子的分词概率是其所有子词概率的乘积。Unigram 的目标是找到使句子概率最大的分词方式。\n",
    "- 动态词汇表：从一个大词汇表开始，逐步删除对整体似然贡献最小的子词，最终得到目标大小的词汇表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手动实现 Unigram tokenization 算法\n",
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "\n",
    "# 统计单词的频率\n",
    "from collections import defaultdict\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "print(word_freqs)\n",
    "\n",
    "# 统计字符的频率\n",
    "char_freqs = defaultdict(int)\n",
    "# 生成每个单词的全部子串，并统计子串的频率\n",
    "subwords_freqs = defaultdict(int)\n",
    "for word, freq in word_freqs.items():\n",
    "    for i in range(len(word)):\n",
    "        char_freqs[word[i]] += freq\n",
    "        # 循环遍历长度至少为2的子字\n",
    "        for j in range(i + 2, len(word) + 1):\n",
    "            subwords_freqs[word[i:j]] += freq\n",
    "# 按频率对子词排序\n",
    "sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "print(char_freqs)\n",
    "print(sorted_subwords[:20])\n",
    "\n",
    "# 将频率靠前的子词 和 单字符 融合在一起，得到一个大小为300的初始词汇表\n",
    "token_freqs = list(char_freqs.items()) + sorted_subwords[: 300 - len(char_freqs)]\n",
    "token_freqs = {token: freq for token, freq in token_freqs}\n",
    "print(list(token_freqs.items())[:10])\n",
    "\n",
    "# 计算所有频率的总和，将频率转化为概率\n",
    "# 相较于小数相乘，对数相加在数值上更稳定，而且这将简化模型损失的计算\n",
    "from math import log\n",
    "total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "print(total_sum)\n",
    "model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}\n",
    "print(list(model.items())[:10])\n",
    "\n",
    "def encode_word(word, model):\n",
    "    # 为词的每一个位置（从 0 开始，一直到词的总长度）都保存一个字典，\n",
    "    # 记录每个位置的最佳分词起点和累计得分。\n",
    "    best_segmentations = [{\"start\": 0, \"score\": 1}] + [\n",
    "        {\"start\": None, \"score\": None} for _ in range(len(word))\n",
    "    ]\n",
    "    # 填充动态规划表\n",
    "    for start_idx in range(len(word)):\n",
    "        # 遍历所有可能的起始位置 start_idx \n",
    "        # best_score_at_start应该由循环的前面的步骤计算和填充\n",
    "        best_score_at_start = best_segmentations[start_idx][\"score\"]\n",
    "        for end_idx in range(start_idx + 1, len(word) + 1):\n",
    "            # 遍历所有可能的结束位置 end_idx，生成子词 token。\n",
    "            token = word[start_idx:end_idx]\n",
    "            if token in model and best_score_at_start is not None:\n",
    "                # 如果子词 token 存在于 model 中，则计算当前分段的累计得\n",
    "                # 分（model[token] + 当前起点的得分）。\n",
    "                score = model[token] + best_score_at_start\n",
    "                # 如果我们发现以 end_idx 结尾的更好分段,我们会更新\n",
    "                # score 是 token 的概率的负对数（-log(probability)），因此分数越小表示概率越大。\n",
    "                if (\n",
    "                    best_segmentations[end_idx][\"score\"] is None\n",
    "                    or best_segmentations[end_idx][\"score\"] > score\n",
    "                ):\n",
    "                    # 保留得分更高的分词路径\n",
    "                    best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score}\n",
    "    # 回溯最优路径\n",
    "    segmentation = best_segmentations[-1]\n",
    "    if segmentation[\"score\"] is None:\n",
    "        # 我们没有找到单词的 tokens  -> unknown\n",
    "        return [\"<unk>\"], None\n",
    "    score = segmentation[\"score\"]\n",
    "    start = segmentation[\"start\"]\n",
    "    # 从单词末尾（len(word)）开始，通过 start 字段逐步回溯到开头，提取所有子词\n",
    "    end = len(word)\n",
    "    tokens = []\n",
    "    while start != 0:\n",
    "        # 将子词按顺序插入单词表头部，确保最终分词顺序正确\n",
    "        tokens.insert(0, word[start:end])\n",
    "        next_start = best_segmentations[start][\"start\"]\n",
    "        end = start\n",
    "        start = next_start\n",
    "    tokens.insert(0, word[start:end])\n",
    "    return tokens, score\n",
    "\n",
    "print(encode_word(\"Hopefully\", model))\n",
    "print(encode_word(\"This\", model))\n",
    "\n",
    "def compute_loss(model):\n",
    "    loss = 0\n",
    "    for word, freq in word_freqs.items():\n",
    "        _, word_loss = encode_word(word, model)\n",
    "        loss += freq * word_loss\n",
    "    return loss\n",
    "\n",
    "import copy\n",
    "def compute_scores(model):\n",
    "    scores = {}\n",
    "    model_loss = compute_loss(model)\n",
    "    for token, score in model.items():\n",
    "        # 我们将保留长度为 1 的 tokens\n",
    "        if len(token) == 1:\n",
    "            continue\n",
    "        model_without_token = copy.deepcopy(model)\n",
    "        _ = model_without_token.pop(token)\n",
    "        scores[token] = compute_loss(model_without_token) - model_loss\n",
    "    return scores\n",
    "\n",
    "# 设置词汇表最终大小为100\n",
    "percent_to_remove = 0.1\n",
    "while len(model) > 100:\n",
    "    scores = compute_scores(model)\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1])\n",
    "    # 删除分数最低的percent_to_remov tokens\n",
    "    for i in range(int(len(model) * percent_to_remove)):\n",
    "        _ = token_freqs.pop(sorted_scores[i][0])\n",
    "    total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}\n",
    "\n",
    "# 对新文本进行 tokenization\n",
    "def tokenize(text, model):\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in words_with_offsets]\n",
    "    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]\n",
    "    return sum(encoded_words, [])\n",
    "\n",
    "\n",
    "print(tokenize(\"This is the Hugging Face course.\", model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模块化构建 tokenizer\n",
    "`Tokenizers`库旨在为 tokenization 每个步骤提供多个选项，你可以任意搭配这些选项。接下来我们将会从零开始构建`tokenizer`（不是基于旧的`tokenizer`训练），"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取语料库\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")\n",
    "def get_training_corpus():\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i : i + 1000][\"text\"]\n",
    "\n",
    "# 离线保存在本地\n",
    "with open(\"wikitext-2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(len(dataset)):\n",
    "        f.write(dataset[i][\"text\"] + \"\\n\")\n",
    "\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "# 创建一个使用 WordPiece 模型的 Tokenizer \n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "# step1 标准化\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
    ")\n",
    "print(tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))\n",
    "\n",
    "# step2 预分词\n",
    "# Whitespace 会使用空格和所有不是字母、数字或下划线的字符进行分割\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "print(tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\"))\n",
    "# WhitespaceSplit 只使用空格进行分割\n",
    "pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "print(pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\"))\n",
    "# 和 normalizer 一样，也可以使用 Sequence 来组合几个预分词的步骤\n",
    "pre_tokenizer = pre_tokenizers.Sequence(\n",
    "    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]\n",
    ")\n",
    "print(pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\"))\n",
    "\n",
    "# step3 通过模型处理输入\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n",
    "# # 如果基于本地文件的话可以按下面的方式训练模型\n",
    "# tokenizer.model = models.WordPiece(unk_token=\"[UNK]\")\n",
    "# tokenizer.train([\"wikitext-2.txt\"], trainer=trainer)\n",
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)\n",
    "\n",
    "# step4 后处理\n",
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "print(cls_token_id, sep_token_id)\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
    ")\n",
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)\n",
    "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences.\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)\n",
    "\n",
    "# step5 指定一个解码器\n",
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")\n",
    "print(tokenizer.decode(encoding.ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
