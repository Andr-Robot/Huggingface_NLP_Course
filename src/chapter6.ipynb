{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Tokenizers库](https://huggingface.co/learn/nlp-course/zh-CN/chapter6)\n",
    "当我们需要微调模型时，我们需要使用与模型预训练相同的`tokenizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于已有的 tokenizer 训练新的 tokenizer\n",
    "大多数`Transformer 模型`使用`子词分词算法`。为了找到语料库中的常见子词，`tokenizer`需要深入统计语料库中的所有文本——这个过程我们称之为`训练（training）`。具体的训练规则取决于使用的`tokenizer`类型。\n",
    "> 训练 tokenizer 与训练模型不同！模型训练使用随机梯度下降使每个 batch 的 loss 小一点。它本质上是随机的（这意味着在即使两次训练的参数和算法完全相同，你也必须设置一些随机数种子才能获得相同的结果）。训练 tokenizer 是一个统计过程，它试图确定哪些子词最适合为给定的语料库选择，确定的过程取决于分词算法。它是确定性的，这意味着在相同的语料库上使用相同的算法进行训练时，得到的结果总是相同的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1 准备语料库\n",
    "from datasets import load_dataset\n",
    "\n",
    "# import os\n",
    "# os.environ[\"http_proxy\"] = \"http://127.0.0.1:1087\"\n",
    "# os.environ[\"https_proxy\"] = \"http://127.0.0.1:1087\"\n",
    "\n",
    "# # 加载数据集\n",
    "raw_datasets = load_dataset(\"code_search_net\", \"python\", trust_remote_code=True)\n",
    "# # raw_datasets[\"train\"]\n",
    "# raw_datasets = load_dataset(\"code-search-net/code_search_net\", \"python\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "tokenizer 对文本进行了的预处理:\n",
    "1. Nomalization: 标准化步骤涉及一些常规清理，例如删除不必要的空格、转小写和“/”或删除重音符号。\n",
    "2. Pre-tokenization: 对原始文本进行初步的分词处理\n",
    "3. Model: \n",
    "4. Postprocessor\n",
    "\n",
    "## 标准化（normalization）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(type(tokenizer.backend_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.backend_tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
