{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Tokenizers库](https://huggingface.co/learn/nlp-course/zh-CN/chapter6)\n",
    "当我们需要微调模型时，我们需要使用与模型预训练相同的`tokenizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于已有的 tokenizer 训练新的 tokenizer\n",
    "大多数`Transformer 模型`使用`子词分词算法`。为了找到语料库中的常见子词，`tokenizer`需要深入统计语料库中的所有文本——这个过程我们称之为`训练（training）`。具体的训练规则取决于使用的`tokenizer`类型。\n",
    "> 训练 tokenizer 与训练模型不同！模型训练使用随机梯度下降使每个 batch 的 loss 小一点。它本质上是随机的（这意味着在即使两次训练的参数和算法完全相同，你也必须设置一些随机数种子才能获得相同的结果）。训练 tokenizer 是一个统计过程，它试图确定哪些子词最适合为给定的语料库选择，确定的过程取决于分词算法。它是确定性的，这意味着在相同的语料库上使用相同的算法进行训练时，得到的结果总是相同的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1 准备语料库\n",
    "from datasets import load_dataset\n",
    "\n",
    "# # 下载不下来数据的时候可以增加魔法\n",
    "# import os\n",
    "# os.environ[\"http_proxy\"] = \"http://127.0.0.1:port\"\n",
    "# os.environ[\"https_proxy\"] = \"http://127.0.0.1:port\"\n",
    "\n",
    "# # 加载数据集\n",
    "raw_datasets = load_dataset(\"code_search_net\", \"python\", trust_remote_code=True)\n",
    "print(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们仅使用 whole_func_string 列来训练我们的 tokenizer \n",
    "# 我们可以通过索引来查看其中一个函数的示例\n",
    "print(raw_datasets[\"train\"][123456][\"whole_func_string\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建迭代器\n",
    "# 将数据集转换为一个文本列表的iterator（迭代器），这样就可以以batch的方式进行训练了\n",
    "# 使用迭代器可以不把所有内容都加载到内存中\n",
    "# 通过 python 的 generator 来实现\n",
    "## version 1 基于列表的 generator\n",
    "def get_training_corpus():\n",
    "    return (\n",
    "        raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
    "        for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    "    )\n",
    "\n",
    "\n",
    "training_corpus = get_training_corpus()\n",
    "print(training_corpus)\n",
    "\n",
    "## version 2 基于 yield 的 generator\n",
    "def get_training_corpus_v2():\n",
    "    dataset = raw_datasets[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"whole_func_string\"]\n",
    "    \n",
    "training_corpus_v2 = get_training_corpus_v2()\n",
    "print(training_corpus_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step2 训练一个新的 tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "# 使用旧的 tokenizer，训练一个新的 tokenizer\n",
    "# 新的 tokenizer 将与 GPT-2 完全相同，唯一的区别是词汇表，这将由我们的语料库通过训练来重新确定。\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "example = '''def add_numbers(a, b):\n",
    "    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n",
    "    return a + b'''\n",
    "\n",
    "tokens = old_tokenizer.tokenize(example)\n",
    "# 展示在旧的 tokenizer 下的 token\n",
    "print(tokens)\n",
    "# 使用code_search_net中的Python数据集，训练一个新的 tokenizer ，词汇表大小为52000\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)\n",
    "tokens = tokenizer.tokenize(example)\n",
    "print(tokens)\n",
    "print(len(tokens))\n",
    "print(len(old_tokenizer.tokenize(example)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token-classification\n",
    "`Token-classification`是自然语言处理（NLP）中的核心任务，旨在为文本中的每个 token（如单词、子词或字符）分配一个特定的标签。它的核心目标是通过对文本的细粒度分析，提取结构化信息或理解语言单元的语义角色，广泛应用于命名实体识别（NER）、分词、词性标注等场景。    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用pipeline，默认基于 dbmdz/bert-large-cased-finetuned-conll03-english 模型对句子进行 NER\n",
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\"token-classification\")\n",
    "print(token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\"))\n",
    "# 可以使用将同一实体的 token 组合在一起\n",
    "## aggregation_strategy 通过设置，可以设置组合后实体计算的策略\n",
    "token_classifier = pipeline(\"token-classification\", aggregation_strategy=\"simple\")\n",
    "print(token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自己构建pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "inputs = tokenizer(example, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "# 输入的token的维度\n",
    "print(inputs[\"input_ids\"].shape)\n",
    "# 输出的维度\n",
    "print(outputs.logits.shape)\n",
    "# 打印索引到标签的映射\n",
    "print(model.config.id2label)\n",
    "\n",
    "import torch\n",
    "# 使用 softmax 函数将这些 logits 转化为概率，并取 argmax 来得到预测\n",
    "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()\n",
    "predictions = outputs.logits.argmax(dim=-1)[0].tolist()\n",
    "print(predictions)\n",
    "\n",
    "# 格式化输出 token 的得分和标签\n",
    "results = []\n",
    "# 获取偏移映射\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        start, end = offsets[idx]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity\": label,\n",
    "                \"score\": probabilities[idx][pred],\n",
    "                \"word\": tokens[idx],\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(results)\n",
    "\n",
    "# 格式化输出组合后 token 的得分和标签\n",
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "idx = 0\n",
    "while idx < len(predictions):\n",
    "    pred = predictions[idx]\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        # 删除 B- 或者 I-\n",
    "        label = label[2:]\n",
    "        start, _ = offsets[idx]\n",
    "\n",
    "        # 获取所有标有 I 标签的token\n",
    "        all_scores = []\n",
    "        while (\n",
    "            idx < len(predictions)\n",
    "            and model.config.id2label[predictions[idx]] == f\"I-{label}\"\n",
    "        ):\n",
    "            all_scores.append(probabilities[idx][pred])\n",
    "            _, end = offsets[idx]\n",
    "            idx += 1\n",
    "\n",
    "        # 分数是该分组实体中所有token分数的平均值\n",
    "        score = np.mean(all_scores).item()\n",
    "        word = example[start:end]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity_group\": label,\n",
    "                \"score\": score,\n",
    "                \"word\": word,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "    idx += 1\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question-answering\n",
    "question-answering pipeline 用于执行问答任务。具体来说，它用于从给定的文本中提取答案，基于一个预训练的模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "context = \"\"\"\n",
    "🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch, and TensorFlow — with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "long_context = \"\"\"\n",
    "🤗 Transformers: State of the Art NLP\n",
    "\n",
    "🤗 Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\n",
    "question answering, summarization, translation, text generation and more in over 100 languages.\n",
    "Its aim is to make cutting-edge NLP easier to use for everyone.\n",
    "\n",
    "🤗 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\n",
    "then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\n",
    "can be modified to enable quick research experiments.\n",
    "\n",
    "Why should I use transformers?\n",
    "\n",
    "1. Easy-to-use state-of-the-art models:\n",
    "  - High performance on NLU and NLG tasks.\n",
    "  - Low barrier to entry for educators and practitioners.\n",
    "  - Few user-facing abstractions with just three classes to learn.\n",
    "  - A unified API for using all our pretrained models.\n",
    "  - Lower compute costs, smaller carbon footprint:\n",
    "\n",
    "2. Researchers can share trained models instead of always retraining.\n",
    "  - Practitioners can reduce compute time and production costs.\n",
    "  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n",
    "\n",
    "3. Choose the right framework for every part of a model's lifetime:\n",
    "  - Train state-of-the-art models in 3 lines of code.\n",
    "  - Move a single model between TF2.0/PyTorch frameworks at will.\n",
    "  - Seamlessly pick the right framework for training, evaluation and production.\n",
    "\n",
    "4. Easily customize a model or an example to your needs:\n",
    "  - We provide examples for each architecture to reproduce the results published by its original authors.\n",
    "  - Model internals are exposed as consistently as possible.\n",
    "  - Model files can be used independently of the library for quick experiments.\n",
    "\n",
    "🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "\n",
    "question = \"Which deep learning libraries back 🤗 Transformers?\"\n",
    "print(question_answerer(question=question, context=context))\n",
    "# 长文本也能处理\n",
    "print(question_answerer(question=question, context=long_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自己构建pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "model_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "# 输入格式是：[CLS] question [SEP] context [SEP] \n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "# 问答模型训练的目标是来预测答案开始的 token 的索引和答案结束的 token 的索引。\n",
    "# 所以返回值是两个logits：一个对应于答案的开始 token 的 logits，另一个对应于答案的结束 token 的 logits。\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(start_logits.shape, end_logits.shape)\n",
    "\n",
    "# 使用 softmax 函数将这些 logits 转化为概率，并取 argmax 来得到预测\n",
    "\"\"\"\n",
    "注意：\n",
    "由于输入格式是 [CLS] question [SEP] context [SEP] ，所以我们需要屏蔽 question 的 tokens 以及 [SEP] token 。\n",
    "不过，我们将保留 [CLS] ，因为某些模型使用它来表示答案不在上下文中。\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "sequence_ids = inputs.sequence_ids()\n",
    "# 屏蔽除 context 之外的所有内容\n",
    "mask = [i != 1 for i in sequence_ids]\n",
    "# 不屏蔽 [CLS] token\n",
    "mask[0] = False\n",
    "mask = torch.tensor(mask)[None]\n",
    "print(mask)\n",
    "# 将想要屏蔽的 logits 替换为一个大的负数\n",
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000\n",
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]\n",
    "print(start_probabilities.shape, end_probabilities.shape)\n",
    "\n",
    "# 在满足 start_index <= end_index 的前提下，计算每个可能的 start_index 和 end_index 的概率，\n",
    "# 然后取概率最高的 (start_index, end_index) 元组。\n",
    "# start_probabilities[start_index] × end_probabilities[end_index] 就是这个 (start_index, end_index) 元组的概率。\n",
    "scores = start_probabilities[:, None] * end_probabilities[None, :]\n",
    "print(scores.shape)\n",
    "import numpy as np\n",
    "# 获取上三角部分\n",
    "scores = torch.triu(scores)\n",
    "\n",
    "# argmax 将返回展平（flattened）后张量中的索引\n",
    "max_index = scores.argmax().item()\n",
    "start_index = max_index // scores.shape[1]\n",
    "end_index = max_index % scores.shape[1]\n",
    "print(start_index, end_index)\n",
    "print(scores[start_index, end_index])\n",
    "\n",
    "# 有了start_index 和 end_index，可以从 context 中提取答案\n",
    "inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "start_char, _ = offsets[start_index]\n",
    "_, end_char = offsets[end_index]\n",
    "answer = context[start_char:end_char]\n",
    "# 格式化输出\n",
    "result = {\n",
    "    \"answer\": answer,\n",
    "    \"start\": start_char,\n",
    "    \"end\": end_char,\n",
    "    \"score\": scores[start_index, end_index],\n",
    "}\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets[start_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_ids = inputs.sequence_ids()\n",
    "sequence_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "tokenizer 对文本进行了的预处理:\n",
    "1. Nomalization: 标准化步骤涉及一些常规清理，例如删除不必要的空格、转小写和“/”或删除重音符号。\n",
    "2. Pre-tokenization: 对原始文本进行初步的分词处理\n",
    "3. Model: \n",
    "4. Postprocessor\n",
    "\n",
    "## 标准化（normalization）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(type(tokenizer.backend_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.backend_tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
