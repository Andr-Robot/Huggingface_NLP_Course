{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [主要的NLP任务](https://huggingface.co/learn/nlp-course/zh-CN/chapter7)\n",
    "- Token 分类\n",
    "- 掩码语言建模（如 BERT）\n",
    "- 文本摘要\n",
    "- 翻译\n",
    "- 因果语言建模预训练（如 GPT-2）\n",
    "- 问答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token 分类\n",
    "目标是对文本中的每个 token（词或子词）进行分类。它是 序列标注（Sequence Labeling） 的核心任务，广泛应用于信息抽取、语法分析和语义理解等领域。常见的任务类型有：\n",
    "- 命名实体识别（NER）：识别文本中的人名、地名、组织名等实体\n",
    "- 词性标注（POS）：标注每个词的语法类别（名词、动词等）\n",
    "- 分块（Chunking）：划分短语边界（如名词短语、动词短语）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# lhoestq/conll2003【其他数据集可能会下载不下来，所有换用这个数据集】\n",
    "raw_datasets = load_dataset(\"lhoestq/conll2003\", trust_remote_code=True)\n",
    "print(raw_datasets)\n",
    "# 查看训练集的第一个元素 和 NER 标签\n",
    "print(raw_datasets[\"train\"][0][\"tokens\"])\n",
    "print(raw_datasets[\"train\"][0][\"ner_tags\"])\n",
    "\n",
    "# 查看 NER 标签的种类\n",
    "ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
    "print(ner_feature)\n",
    "label_names = ner_feature.feature.names\n",
    "print(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = raw_datasets[\"train\"][0][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 tokenizer 对象\n",
    "from transformers import AutoTokenizer\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "# is_split_into_words 用于指定输入文本是否已经预先分词（例如，按空格分割成单词）。\n",
    "# 如果设置为 True，则分词器假定输入已经分割成单词，并对这些单词进行进一步的处理\n",
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
    "print(inputs)\n",
    "print(inputs.tokens())\n",
    "print(\"input tokens size:%d\" % len(raw_datasets[\"train\"][0][\"tokens\"]))\n",
    "print(\"after tokenized size:%d\" % len(inputs.tokens()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    \"\"\"\n",
    "    将 NER 标签与分词后的 tokens 对齐\n",
    "    args:\n",
    "        labels (List[int]): NER 标签\n",
    "        word_ids (List[int]): 分词后的 tokens\n",
    "    return:\n",
    "        new_labels (List[int]): 对齐后的标签\n",
    "    \"\"\"\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # 新单词的开始!\n",
    "            current_word = word_id\n",
    "            # 将特殊的 token 设置为 -100\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # 特殊的token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # 与前一个 tokens 类型相同的单词\n",
    "            label = labels[word_id]\n",
    "            # 如果标签是 B-XXX 我们将其更改为 I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "    return new_labels\n",
    "\n",
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(word_ids)\n",
    "print(len(word_ids))\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"\n",
    "    对数据集进行分词，并将 NER 标签与分词后的 tokens 对齐\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "# 对齐数据集中的所有数据\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# step1 整理数据\n",
    "# DataCollatorForTokenClassification 带有填充功能的Data Collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "print(batch[\"labels\"])\n",
    "# 对比数据整理器的结果与数据集中未经处理的结果\n",
    "for i in range(2):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])\n",
    "# step2 评估指标\n",
    "import evaluate\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "labels = [label_names[i] for i in labels]\n",
    "print(labels)\n",
    "# 更改索引 2 处的值，创建假的预测值，然后测试评估指标\n",
    "predictions = labels.copy()\n",
    "predictions[2] = \"O\"\n",
    "print(metric.compute(predictions=[predictions], references=[labels]))\n",
    "import numpy as np\n",
    "#  自定义 compute_metrics() 函数，返回所需要的指标\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # 删除忽略的索引(特殊 tokens )并转换为标签\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# step3 定义模型\n",
    "id2label = {str(i): label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "print(\"num_labels=%d\" % model.config.num_labels)\n",
    "\n",
    "# step4 微调模型\n",
    "from transformers import TrainingArguments, Trainer\n",
    "# 设置 TrainingArguments\n",
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义训练循环\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator\n",
    "from transformers import get_scheduler\n",
    "\n",
    "# 为数据集构建 DataLoader \n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=8\n",
    ")\n",
    "\n",
    "# 初始化模型\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# 初始化优化器\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# 将模型和优化器发送到 Accelerator，Accelerator 简化和优化分布式训练及硬件加速\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "# 使用线性学习率调度器\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "model_name = \"bert-finetuned-ner-accelerate\"\n",
    "\n",
    "# 计算 metric\n",
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "    # 删除忽略的索引(特殊 tokens )并转换为标签\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_labels, true_predictions\n",
    "\n",
    "# 初始化损失记录\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "for epoch in range(num_train_epochs):\n",
    "    # 训练\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()  # 累加训练损失\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "    # 计算平均训练损失\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    # 评估\n",
    "    model.eval()\n",
    "    total_eval_loss = 0.0\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_eval_loss += loss.item()  # 累加验证损失\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "        # 填充模型的预测和标签后才能调用 gathere()\n",
    "        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "        predictions_gathered = accelerator.gather(predictions)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "    # 计算平均验证损失\n",
    "    avg_eval_loss = total_eval_loss / len(eval_dataloader)\n",
    "    eval_losses.append(avg_eval_loss)\n",
    "    results = metric.compute()\n",
    "    print(\n",
    "        f\"epoch {epoch}:\",\n",
    "        {\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"eval_loss\": avg_eval_loss,\n",
    "            \"precision\": results[f\"overall_precision\"],\n",
    "            \"recall\": results[f\"overall_recall\"],\n",
    "            \"f1\": results[f\"overall_f1\"],\n",
    "            \"accuracy\": results[f\"overall_accuracy\"]\n",
    "        },\n",
    "    )\n",
    "    # 保存模型\n",
    "    output_dir = f\"model/{model_name}-epoch{epoch}\"\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(\n",
    "        output_dir,\n",
    "        is_main_process=accelerator.is_main_process,\n",
    "        save_function=accelerator.save\n",
    "    )\n",
    "    # 同时保存训练状态\n",
    "    accelerator.save(\n",
    "        {\"epoch\": epoch, \"optimizer_state\": optimizer.state_dict(), \"lr_scheduler_state\": lr_scheduler.state_dict()}, \n",
    "        f\"{output_dir}/training_state.pt\"\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        print(f\"\\nModel saved to {output_dir}\")\n",
    "\n",
    "# \"\"\"\n",
    "# 保存最终的模型\n",
    "# \"\"\"\n",
    "# # 全部训练完成后保存最终模型\n",
    "# output_dir = f\"{model_name}-final\"\n",
    "# accelerator.wait_for_everyone()\n",
    "# unwrapped_model = accelerator.unwrap_model(model)\n",
    "# unwrapped_model.save_pretrained(\n",
    "#     output_dir,\n",
    "#     is_main_process=accelerator.is_main_process,\n",
    "#     save_function=accelerator.save\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化加速器\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# 1. 加载预训练的tokenizer和模型\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"model/bert-finetuned-ner-accelerate-epoch2\",\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# 2. 加载测试集\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"test\"],\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "# 3. 准备测试集和模型\n",
    "model, test_dataloader = accelerator.prepare(model, test_dataloader)\n",
    "\n",
    "# 4. 预测测试集并计算指标\n",
    "model.eval()\n",
    "# 准备存储预测结果和真实标签\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "# 迭代测试集\n",
    "progress_test = tqdm(test_dataloader, desc=\"Testing\")\n",
    "for batch in progress_test:\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    predictions = outputs.logits.argmax(dim=-1)\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    # 填充模型的预测和标签后才能调用 gathere()\n",
    "    predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "    labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "    predictions_gathered = accelerator.gather(predictions)\n",
    "    labels_gathered = accelerator.gather(labels)\n",
    "    true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "    # 添加到列表中\n",
    "    all_predictions.extend(true_predictions)\n",
    "    all_labels.extend(true_labels)\n",
    "    metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "results = metric.compute()\n",
    "print(\n",
    "        f\"epoch {epoch}:\",\n",
    "        {\n",
    "            key: results[f\"overall_{key}\"]\n",
    "            for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n",
    "        },\n",
    "    )\n",
    "\n",
    "# 5. 保存预测结果到本地文件\n",
    "import json\n",
    "with open(\"test_predictions.json\", \"w\") as f:\n",
    "    json.dump(all_predictions, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过 token_classifier pipeline 测试模型\n",
    "from transformers import pipeline\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"model/bert-finetuned-ner-accelerate-epoch2\"\n",
    ")\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\"\n",
    ")\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 微调掩码语言模型（masked language model）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
