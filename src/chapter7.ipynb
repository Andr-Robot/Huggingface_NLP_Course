{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [主要的NLP任务](https://huggingface.co/learn/nlp-course/zh-CN/chapter7)\n",
    "- Token 分类\n",
    "- 掩码语言建模（如 BERT）\n",
    "- 文本摘要\n",
    "- 翻译\n",
    "- 因果语言建模预训练（如 GPT-2）\n",
    "- 问答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token 分类\n",
    "目标是对文本中的每个 token（词或子词）进行分类。它是 序列标注（Sequence Labeling） 的核心任务，广泛应用于信息抽取、语法分析和语义理解等领域。常见的任务类型有：\n",
    "- 命名实体识别（NER）：识别文本中的人名、地名、组织名等实体\n",
    "- 词性标注（POS）：标注每个词的语法类别（名词、动词等）\n",
    "- 分块（Chunking）：划分短语边界（如名词短语、动词短语）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# lhoestq/conll2003【其他数据集可能会下载不下来，所有换用这个数据集】\n",
    "raw_datasets = load_dataset(\"lhoestq/conll2003\", trust_remote_code=True)\n",
    "print(raw_datasets)\n",
    "# 查看训练集的第一个元素 和 NER 标签\n",
    "print(raw_datasets[\"train\"][0][\"tokens\"])\n",
    "print(raw_datasets[\"train\"][0][\"ner_tags\"])\n",
    "\n",
    "# 查看 NER 标签的种类\n",
    "ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
    "print(ner_feature)\n",
    "label_names = ner_feature.feature.names\n",
    "print(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = raw_datasets[\"train\"][0][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 tokenizer 对象\n",
    "from transformers import AutoTokenizer\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "# is_split_into_words 用于指定输入文本是否已经预先分词（例如，按空格分割成单词）。\n",
    "# 如果设置为 True，则分词器假定输入已经分割成单词，并对这些单词进行进一步的处理\n",
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
    "print(inputs)\n",
    "print(inputs.tokens())\n",
    "print(\"input tokens size:%d\" % len(raw_datasets[\"train\"][0][\"tokens\"]))\n",
    "print(\"after tokenized size:%d\" % len(inputs.tokens()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    \"\"\"\n",
    "    将 NER 标签与分词后的 tokens 对齐\n",
    "    args:\n",
    "        labels (List[int]): NER 标签\n",
    "        word_ids (List[int]): 分词后的 tokens\n",
    "    return:\n",
    "        new_labels (List[int]): 对齐后的标签\n",
    "    \"\"\"\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # 新单词的开始!\n",
    "            current_word = word_id\n",
    "            # 将特殊的 token 设置为 -100\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # 特殊的token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # 与前一个 tokens 类型相同的单词\n",
    "            label = labels[word_id]\n",
    "            # 如果标签是 B-XXX 我们将其更改为 I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "    return new_labels\n",
    "\n",
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(word_ids)\n",
    "print(len(word_ids))\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"\n",
    "    对数据集进行分词，并将 NER 标签与分词后的 tokens 对齐\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs\n",
    "# 对齐数据集中的所有数据\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# step1 整理数据\n",
    "# DataCollatorForTokenClassification 带有填充功能的Data Collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "print(batch[\"labels\"])\n",
    "# 对比数据整理器的结果与数据集中未经处理的结果\n",
    "for i in range(2):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])\n",
    "# step2 评估指标\n",
    "import evaluate\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "labels = raw_datasets[\"train\"][0][\"ner_tags\"]\n",
    "labels = [label_names[i] for i in labels]\n",
    "print(labels)\n",
    "# 更改索引 2 处的值，创建假的预测值，然后测试评估指标\n",
    "predictions = labels.copy()\n",
    "predictions[2] = \"O\"\n",
    "print(metric.compute(predictions=[predictions], references=[labels]))\n",
    "import numpy as np\n",
    "#  自定义 compute_metrics() 函数，返回所需要的指标\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    # 删除忽略的索引(特殊 tokens )并转换为标签\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# step3 定义模型\n",
    "id2label = {str(i): label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "print(\"num_labels=%d\" % model.config.num_labels)\n",
    "\n",
    "# step4 微调模型\n",
    "from transformers import TrainingArguments, Trainer\n",
    "# 设置 TrainingArguments\n",
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义训练循环\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator\n",
    "from transformers import get_scheduler\n",
    "\n",
    "# 为数据集构建 DataLoader \n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=8\n",
    ")\n",
    "\n",
    "# 初始化模型\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# 初始化优化器\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# 将模型和优化器发送到 Accelerator，Accelerator 简化和优化分布式训练及硬件加速\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "# 使用线性学习率调度器\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "model_name = \"bert-finetuned-ner-accelerate\"\n",
    "\n",
    "# 计算 metric\n",
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "    # 删除忽略的索引(特殊 tokens )并转换为标签\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_labels, true_predictions\n",
    "\n",
    "# 初始化损失记录\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "for epoch in range(num_train_epochs):\n",
    "    # 训练\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()  # 累加训练损失\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "    # 计算平均训练损失\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    # 评估\n",
    "    model.eval()\n",
    "    total_eval_loss = 0.0\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_eval_loss += loss.item()  # 累加验证损失\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "        # 填充模型的预测和标签后才能调用 gathere()\n",
    "        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "        predictions_gathered = accelerator.gather(predictions)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "    # 计算平均验证损失\n",
    "    avg_eval_loss = total_eval_loss / len(eval_dataloader)\n",
    "    eval_losses.append(avg_eval_loss)\n",
    "    results = metric.compute()\n",
    "    print(\n",
    "        f\"epoch {epoch}:\",\n",
    "        {\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"eval_loss\": avg_eval_loss,\n",
    "            \"precision\": results[f\"overall_precision\"],\n",
    "            \"recall\": results[f\"overall_recall\"],\n",
    "            \"f1\": results[f\"overall_f1\"],\n",
    "            \"accuracy\": results[f\"overall_accuracy\"]\n",
    "        },\n",
    "    )\n",
    "    # 保存模型\n",
    "    output_dir = f\"../model/{model_name}-epoch{epoch}\"\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(\n",
    "        output_dir,\n",
    "        is_main_process=accelerator.is_main_process,\n",
    "        save_function=accelerator.save\n",
    "    )\n",
    "    # 同时保存训练状态\n",
    "    accelerator.save(\n",
    "        {\"epoch\": epoch, \"optimizer_state\": optimizer.state_dict(), \"lr_scheduler_state\": lr_scheduler.state_dict()}, \n",
    "        f\"{output_dir}/training_state.pt\"\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        print(f\"\\nModel saved to {output_dir}\")\n",
    "\n",
    "# \"\"\"\n",
    "# 保存最终的模型\n",
    "# \"\"\"\n",
    "# # 全部训练完成后保存最终模型\n",
    "# output_dir = f\"{model_name}-final\"\n",
    "# accelerator.wait_for_everyone()\n",
    "# unwrapped_model = accelerator.unwrap_model(model)\n",
    "# unwrapped_model.save_pretrained(\n",
    "#     output_dir,\n",
    "#     is_main_process=accelerator.is_main_process,\n",
    "#     save_function=accelerator.save\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化加速器\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# 1. 加载预训练的模型\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"../model/bert-finetuned-ner-accelerate-epoch2\",\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# 2. 加载测试集\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"test\"],\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "# 3. 准备测试集和模型\n",
    "model, test_dataloader = accelerator.prepare(model, test_dataloader)\n",
    "\n",
    "# 4. 预测测试集并计算指标\n",
    "model.eval()\n",
    "# 准备存储预测结果和真实标签\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "# 迭代测试集\n",
    "progress_test = tqdm(test_dataloader, desc=\"Testing\")\n",
    "for batch in progress_test:\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    predictions = outputs.logits.argmax(dim=-1)\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    # 填充模型的预测和标签后才能调用 gathere()\n",
    "    predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "    labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "    predictions_gathered = accelerator.gather(predictions)\n",
    "    labels_gathered = accelerator.gather(labels)\n",
    "    true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "    # 添加到列表中\n",
    "    all_predictions.extend(true_predictions)\n",
    "    all_labels.extend(true_labels)\n",
    "    metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "results = metric.compute()\n",
    "print(\n",
    "        {\n",
    "            key: results[f\"overall_{key}\"]\n",
    "            for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n",
    "        },\n",
    "    )\n",
    "\n",
    "# 5. 保存预测结果到本地文件\n",
    "import json\n",
    "with open(\"test_predictions.json\", \"w\") as f:\n",
    "    json.dump(all_predictions, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过 token_classifier pipeline 测试模型\n",
    "from transformers import pipeline\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"../model/bert-finetuned-ner-accelerate-epoch2\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\"\n",
    ")\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 微调掩码语言模型（masked language model）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "# distilbert模型训练的语料数据主要来源于维基百科\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "# 通过 num_parameters 方法获取模型参数数量\n",
    "distilbert_num_parameters = model.num_parameters() / 1_000_000\n",
    "print(f\"'>>> DistilBERT number of parameters: {round(distilbert_num_parameters)}M'\")\n",
    "print(f\"'>>> BERT number of parameters: 110M'\")\n",
    "\n",
    "# 测试模型能力\n",
    "text = \"This is a great [MASK].\"\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "import torch\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "token_logits = model(**inputs).logits\n",
    "# 找到 [MASK] 的位置并提取其 logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# 选择具有最高 logits 的 [MASK] 候选词\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")\n",
    "\n",
    "# 通过使用 IMDB 的大型电影评论数据集对 DistilBERT 进行微调，期望模型能输出电影评论领域相关的预测结果\n",
    "from datasets import load_dataset\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "print(imdb_dataset)\n",
    "\n",
    "# 创建随机样本\n",
    "sample = imdb_dataset[\"train\"].shuffle(seed=42).select(range(3))\n",
    "for row in sample:\n",
    "    print(f\"\\n'>>> Review: {row['text']}'\")\n",
    "    print(f\"'>>> Label: {row['label']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理数据\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "# 使用 batched=True 来激活快速多线程!\n",
    "tokenized_datasets = imdb_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\n",
    ")\n",
    "print(tokenized_datasets)\n",
    "\n",
    "# 切片会为每个特征生成一个列表的列表\n",
    "tokenized_samples = tokenized_datasets[\"train\"][:3]\n",
    "# 打印出每个评论的 token 数量\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"'>>> Review {idx} length: {len(sample)}'\")\n",
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated reviews length: {total_length}'\")\n",
    "\n",
    "# 将连接的评论拆分为大小为 chunk_size 的块\n",
    "# 最后一个块通常会小于所设置的分块的大小。有两种常见的策略来处理这个问题：\n",
    "# 1. 如果最后一个块小于 chunk_size ，就丢弃。\n",
    "# 2. 填充最后一个块，直到其长度等于 chunk_size 。\n",
    "chunk_size = 128\n",
    "chunks = {\n",
    "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "}\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")\n",
    "\n",
    "def group_texts(examples):\n",
    "    \"\"\"\n",
    "    将所有文本拼接在一起，然后分块\n",
    "    \"\"\"\n",
    "    # 拼接所有的文本\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # 计算拼接文本的长度\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # 如果最后一个块小于 chunk_size,我们将其丢弃\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # 按最大长度分块\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # 创建一个新的 labels 列\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 Trainer API 微调 DistilBERT\n",
    "\n",
    "# 随机掩码 \n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "# 通过DataCollatorForLanguageModeling，使用参数mlm_probability（掩盖 tokens 的比例）实现\n",
    "\"\"\"\n",
    "随机掩码的一个缺点是，当使用 Trainer 时，每次计算出来的评估结果会有些许不同，即使\n",
    "我们会对训练集和测试集使用相同的数据整理器。\n",
    "\"\"\"\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")\n",
    "\n",
    "# 全词屏蔽（whole word masking），可以一次遮蔽整个单词的所有 token，实现\n",
    "# 全词屏蔽需要自己构建一个数据整理器\n",
    "import collections\n",
    "import numpy as np\n",
    "from transformers import default_data_collator\n",
    "wwm_probability = 0.2\n",
    "\"\"\"\n",
    "将使用先前计算的word ID，构建一个单词索引和相应 token 之间的映射，然后随机决定遮蔽哪些单\n",
    "词，并使用这种方法对输入进行遮蔽。请注意，除了与掩码对应的标签外，所有其他的标签均应该设置\n",
    "为 -100 \n",
    "\"\"\"\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "        # 创建一个单词与对应 token 索引之间的映射\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "        # 随机遮蔽单词\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        feature[\"labels\"] = new_labels\n",
    "    return default_data_collator(features)\n",
    "\n",
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 考虑到硬件能力，这里筛选少量数据来训练\n",
    "train_size = 10_000\n",
    "test_size = int(0.1 * train_size)\n",
    "downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=42\n",
    ")\n",
    "print(downsampled_dataset)\n",
    "\n",
    "# 指定Trainer参数\n",
    "from transformers import TrainingArguments\n",
    "batch_size = 64\n",
    "# 在每个 epoch 输出训练的 loss\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "# tips: mac 机器上可能需要禁用混合精度训练\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"../model/{model_name}-finetuned-imdb\",\n",
    "    overwrite_output_dir=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    fp16=False,    # 禁用FP16\n",
    "    bf16=False,    # 禁用BF16\n",
    "    no_cuda=True,  # 确保不使用CUDA（即使有GPU）\n",
    "    logging_steps=logging_steps,\n",
    ")\n",
    "# 强制使用 CPU 或 CUDA\n",
    "device = torch.device(\"cpu\")  # 或 \"cuda\"/\"mps\"\n",
    "model = model.to(device)\n",
    "# 创建Trainer\n",
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "# 指定评估指标 Perplexity\n",
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "trainer.train()\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 Accelerate 微调 DistilBERT\n",
    "\"\"\"\n",
    "DataCollatorForLanguageModeling 在每次评估时也会进行随机遮罩，因此我们在每次训练运行\n",
    "中都会看到困惑度得分有些波动。\n",
    "因此在整个测试集上 仅进行一次 遮罩，以确保我们的评估指标是一致的。\n",
    "\"\"\"\n",
    "def insert_random_mask(batch):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = data_collator(features)\n",
    "    # 为数据集中的每一列创建一个新的\"masked\"列\n",
    "    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}\n",
    "downsampled_dataset = downsampled_dataset.remove_columns([\"word_ids\"])\n",
    "eval_dataset = downsampled_dataset[\"test\"].map(\n",
    "    insert_random_mask,\n",
    "    batched=True,\n",
    "    remove_columns=downsampled_dataset[\"test\"].column_names,\n",
    ")\n",
    "eval_dataset = eval_dataset.rename_columns(\n",
    "    {\n",
    "        \"masked_input_ids\": \"input_ids\",\n",
    "        \"masked_attention_mask\": \"attention_mask\",\n",
    "        \"masked_labels\": \"labels\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# 设置 DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(\n",
    "    downsampled_dataset[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, batch_size=batch_size, collate_fn=default_data_collator\n",
    ")\n",
    "\n",
    "# 重新加载预训练模型\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "# 指定优化器\n",
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 用 Accelerator 对象包装所有的组件\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "\n",
    "# 设置学习率调度器\n",
    "from transformers import get_scheduler\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-imdb-accelerate\"\n",
    "\n",
    "# 训练和评估的循环\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import math\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "for epoch in range(num_train_epochs):\n",
    "    # 训练\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "    # 评估\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_dataset)]\n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
    "    # 保存模型\n",
    "    output_dir = f\"../model/{model_name}-epoch{epoch}\"\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(\n",
    "        output_dir,\n",
    "        is_main_process=accelerator.is_main_process,\n",
    "        save_function=accelerator.save\n",
    "    )\n",
    "    # 同时保存训练状态\n",
    "    accelerator.save(\n",
    "        {\"epoch\": epoch, \"optimizer_state\": optimizer.state_dict(), \"lr_scheduler_state\": lr_scheduler.state_dict()}, \n",
    "        f\"{output_dir}/training_state.pt\"\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        print(f\"\\nModel saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用我们微调的模型\n",
    "from transformers import pipeline\n",
    "model_checkpoint = \"../model/distilbert-base-uncased-finetuned-imdb-accelerate-epoch1\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "mask_filler = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
    "text = \"This is a great [MASK].\"\n",
    "preds = mask_filler(text)\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 翻译\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 KDE4 数据集\n",
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")\n",
    "print(raw_datasets)\n",
    "# 数据集拆分\n",
    "split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=20)\n",
    "print(split_datasets)\n",
    "split_datasets[\"validation\"] = split_datasets.pop(\"test\")\n",
    "print(split_datasets[\"train\"][1][\"translation\"])\n",
    "\n",
    "# 加载预训练模型\n",
    "from transformers import pipeline\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "translator = pipeline(\"translation\", model=model_checkpoint)\n",
    "print(translator(\"Default to expanded threads\"))\n",
    "\n",
    "# 加载分词器\n",
    "from transformers import AutoTokenizer\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")\n",
    "en_sentence = split_datasets[\"train\"][1][\"translation\"][\"en\"]\n",
    "fr_sentence = split_datasets[\"train\"][1][\"translation\"][\"fr\"]\n",
    "inputs = tokenizer(en_sentence, text_target=fr_sentence)\n",
    "# 输出包含了英语句子的 inputs IDs，而法语句子的 IDs 存储在 labels 字段中\n",
    "print(inputs)\n",
    "\n",
    "# 预处理数据集\n",
    "max_length = 128\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, text_target=targets, max_length=max_length, truncation=True\n",
    "    )\n",
    "    return model_inputs\n",
    "tokenized_datasets = split_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split_datasets[\"train\"].column_names,\n",
    ")\n",
    "\n",
    "# 使用 Trainer API 微调模型\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "# 测试data collator\n",
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(1, 3)])\n",
    "print(batch.keys())\n",
    "print(batch[\"labels\"])\n",
    "print(batch[\"decoder_input_ids\"])\n",
    "for i in range(1, 3):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])\n",
    "  \n",
    "# 评估指标\n",
    "import evaluate\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "# 测试评估指标\n",
    "predictions = [\n",
    "    \"This plugin lets you translate web pages between several languages automatically.\"\n",
    "]\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "print(metric.compute(predictions=predictions, references=references))\n",
    "predictions = [\"This plugin\"]\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "print(metric.compute(predictions=predictions, references=references))\n",
    "# 需要清理标签中的所有 -100 token，为了方便输出的结果直接用来计算评估指标\n",
    "import numpy as np\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # 如果模型返回的内容超过了预测的logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # 由于我们无法解码 -100,因此将标签中的 -100 替换掉\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # 一些简单的后处理\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}\n",
    "\n",
    "# 初始化 TrainingArguments\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"../model/marian-finetuned-kde4-en-to-fr\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# 初始化 Trainer\n",
    "from transformers import Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 开始训练之前，先查看一下模型目前的 BLEU 分数\n",
    "print(trainer.evaluate(max_length=max_length))\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()\n",
    "\n",
    "# 再次评估模型效果\n",
    "print(trainer.evaluate(max_length=max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义训练循环\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 将数据集设置为 torch 格式\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=8\n",
    ")\n",
    "\n",
    "# 实例化model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "# 设置优化器\n",
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# 用 Accelerator 对象包装所有的组件\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "\n",
    "# 设置学习率调度器\n",
    "from transformers import get_scheduler\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "model_name = \"marian-finetuned-kde4-en-to-fr-accelerate\"\n",
    "\n",
    "# 为了简化评估部分，这里定义了这个 postprocess() 函数，该函数接受 predictions 和 labels 并\n",
    "# 将它们转换为 metric 对象所需的字符串列表：\n",
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # 替换标签中的 -100,因为我们无法解码它们。\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # 一些简单的后处理\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    return decoded_preds, decoded_labels\n",
    "  \n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "for epoch in range(num_train_epochs):\n",
    "    # 训练\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "    # 评估\n",
    "    model.eval()\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                max_length=128,\n",
    "            )\n",
    "        labels = batch[\"labels\"]\n",
    "        # 需要填充预测和标签才能调用gather()\n",
    "        generated_tokens = accelerator.pad_across_processes(\n",
    "            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "        predictions_gathered = accelerator.gather(generated_tokens)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "    results = metric.compute()\n",
    "    print(f\"epoch {epoch}, BLEU score: {results['score']:.2f}\")\n",
    "    # 保存模型\n",
    "    output_dir = f\"../model/{model_name}-epoch{epoch}\"\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(\n",
    "        output_dir,\n",
    "        is_main_process=accelerator.is_main_process,\n",
    "        save_function=accelerator.save\n",
    "    )\n",
    "    # 同时保存训练状态\n",
    "    accelerator.save(\n",
    "        {\"epoch\": epoch, \"optimizer_state\": optimizer.state_dict(), \"lr_scheduler_state\": lr_scheduler.state_dict()}, \n",
    "        f\"{output_dir}/training_state.pt\"\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        print(f\"\\nModel saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用微调后的模型\n",
    "from transformers import pipeline\n",
    "# 将其替换成你自己的 checkpoint\n",
    "model_checkpoint = \"../model/marian-finetuned-kde4-en-to-fr-epoch0\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "translator = pipeline(\"translation\", model=model, tokenizer=tokenizer)\n",
    "print(translator(\"Default to expanded threads\"))\n",
    "print(translator(\n",
    "    \"Unable to import %1 using the OFX importer plugin. This file is not the correct format.\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取文本摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# 加载数据集 多语言亚马逊评论语料库 ，注：huggingface上数据集没有了，需要从kaggle下载\n",
    "# spanish_dataset = load_dataset(\"mteb/amazon_reviews_multi\", \"es\", trust_remote_code=True)\n",
    "# english_dataset = load_dataset(\"mteb/amazon_reviews_multi\", \"en\", trust_remote_code=True)\n",
    "# 从kaggle(https://www.kaggle.com/datasets/mexwell/amazon-reviews-multi/data)下载后，本地加载\n",
    "data_path = \"~/.cache/huggingface/datasets/amazon_reviews_multi/\"\n",
    "dataset = load_dataset('csv', data_files={\n",
    "    'train': '%s/train.csv' % data_path,\n",
    "    'validation': '%s/validation.csv' % data_path,\n",
    "    'test': '%s/test.csv' % data_path\n",
    "})\n",
    "# 只获取指定语言类型的评论\n",
    "def filter_language(example, language):\n",
    "    return (\n",
    "        example[\"language\"] == language\n",
    "    )\n",
    "spanish_dataset = dataset.filter(lambda example: filter_language(example, \"es\"))\n",
    "english_dataset = dataset.filter(lambda example: filter_language(example, \"en\"))\n",
    "print(spanish_dataset)\n",
    "print(english_dataset)\n",
    "\n",
    "# 随机抽取部分数据来查看内容\n",
    "def show_samples(dataset, num_samples=3, seed=42):\n",
    "    sample = dataset[\"train\"].shuffle(seed=seed).select(range(num_samples))\n",
    "    for example in sample:\n",
    "        print(f\"\\n'>> Title: {example['review_title']}'\")\n",
    "        print(f\"'>> Review: {example['review_body']}'\")\n",
    "show_samples(english_dataset)\n",
    "\n",
    "# 只获取产品类别为书籍的评论，减少训练时间\n",
    "def filter_books(example):\n",
    "    return (\n",
    "        example[\"product_category\"] == \"book\"\n",
    "        or example[\"product_category\"] == \"digital_ebook_purchase\"\n",
    "    )\n",
    "spanish_books = spanish_dataset.filter(filter_books)\n",
    "english_books = english_dataset.filter(filter_books)\n",
    "show_samples(english_books)\n",
    "\n",
    "# 将英文和西班牙文评论作为单个 DatasetDict 对象组合起来\n",
    "from datasets import concatenate_datasets, DatasetDict\n",
    "books_dataset = DatasetDict()\n",
    "for split in english_books.keys():\n",
    "    books_dataset[split] = concatenate_datasets(\n",
    "        [english_books[split], spanish_books[split]]\n",
    "    )\n",
    "    books_dataset[split] = books_dataset[split].shuffle(seed=42)\n",
    "show_samples(books_dataset)\n",
    "\n",
    "# 过滤掉标题非常短的示例，以便模型可以生成更有效的摘要\n",
    "books_dataset = books_dataset.filter(lambda x: len(x[\"review_title\"].split()) > 2)\n",
    "\n",
    "# 预处理数据\n",
    "from transformers import AutoTokenizer\n",
    "model_checkpoint = \"google/mt5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "inputs = tokenizer(\"I loved reading the Hunger Games!\")\n",
    "print(inputs)\n",
    "print(tokenizer.convert_ids_to_tokens(inputs.input_ids))\n",
    "# 设置评论和标题的最大长度\n",
    "max_input_length = 512\n",
    "max_target_length = 30\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"review_body\"],\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        examples[\"review_title\"], max_length=max_target_length, truncation=True\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "tokenized_datasets = books_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# 评估指标\n",
    "import evaluate\n",
    "rouge_score = evaluate.load(\"rouge\")\n",
    "# 测试评估指标\n",
    "generated_summary = \"I absolutely loved reading the Hunger Games\"\n",
    "reference_summary = \"I loved reading the Hunger Games\"\n",
    "scores = rouge_score.compute(\n",
    "    predictions=[generated_summary], references=[reference_summary]\n",
    ")\n",
    "print(scores)\n",
    "\n",
    "# 创建强大的 baseline\n",
    "# 对于文本摘要，一个常见的参考 baseline 是简单地取文章的前三句话作为摘要，通常\n",
    "# 称为 lead-3 baseline\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "def three_sentence_summary(text):\n",
    "    return \"\\n\".join(sent_tokenize(text)[:3])\n",
    "print(three_sentence_summary(books_dataset[\"train\"][1][\"review_body\"]))\n",
    "# 从数据集中提取这些“摘要”并计算 baseline 的 ROUGE 分数\n",
    "def evaluate_baseline(dataset, metric):\n",
    "    summaries = [three_sentence_summary(text) for text in dataset[\"review_body\"]]\n",
    "    return metric.compute(predictions=summaries, references=dataset[\"review_title\"])\n",
    "\n",
    "# 计算验证集上的 ROUGE 分数\n",
    "import pandas as pd\n",
    "score = evaluate_baseline(books_dataset[\"validation\"], rouge_score)\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "rouge_dict = {rn: round(score[rn] * 100, 2) for rn in rouge_names}\n",
    "print(rouge_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 Trainer API 微调 mT5\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "# 加载预训练模型\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "# 初始化 TrainingArguments\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "batch_size = 4\n",
    "num_train_epochs = 8\n",
    "# 每个训练周期都输出训练损失\n",
    "logging_steps = len(tokenized_datasets[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"../model/{model_name}-finetuned-amazon-en-es\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5.6e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=logging_steps,\n",
    "    gradient_accumulation_steps=64  # 累积 64 步后更新参数\n",
    ")\n",
    "\n",
    "# 模型评估指标 \n",
    "# 对于摘要模型来说，不能直接调用 rouge_score.compute() 进行评估，因为需要将输出和参考摘要解码为文\n",
    "# 本，然后才能计算 ROUGE 分数\n",
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # 将生成的摘要解码为文本\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # 替换标签中的-100,因为我们无法解码它们\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # 将参考摘要解码为文本\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # ROUGE期望每个句子后都有一个换行符\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    # 计算ROUGE分数\n",
    "    result = rouge_score.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # 计算ROUGE分数\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "# 初始化数据整理器（data collator）\n",
    "\"\"\"\n",
    "由于 mT5 是一个编码器-解码器的 Transformer 模型，因此在将数据整理成 batch 时有一点需要注意，那就是\n",
    "在解码期间，我们需要将标签向右移动一个单位。这是为了确保解码器只看到之前的参考序列，而不是当前要预测的 token 或\n",
    "之后的参考序列，这样模型就能避免容易记住标签。\n",
    "\"\"\"\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(\n",
    "    books_dataset[\"train\"].column_names\n",
    ")\n",
    "features = [tokenized_datasets[\"train\"][i] for i in range(2)]\n",
    "print(data_collator(features))\n",
    "\n",
    "# 初始化 Trainer\n",
    "from transformers import Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "# 开始训练\n",
    "trainer.train()\n",
    "# 评估模型\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 Accelerate 微调 mT5\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "from torch.utils.data import DataLoader\n",
    "# DataLoader\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=batch_size\n",
    ")\n",
    "\n",
    "# 优化器\n",
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# 实例化model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "# 使用 Accelerator 对象包装所有的组件\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "\n",
    "# 设置学习率调度器\n",
    "from transformers import get_scheduler\n",
    "num_train_epochs = 10\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "# 定义后处理函数\n",
    "# 将生成的摘要拆分为由换行符分隔的句子，这是 ROUGE 指标需要的输入格式\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "    # ROUGE 需要每个句子后有一个换行符\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "model_name = \"test-bert-finetuned-squad-accelerate\"\n",
    "\n",
    "# 训练和评估的循环\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "for epoch in range(num_train_epochs):\n",
    "    # 训练\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "    # 评估\n",
    "    model.eval()\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "            )\n",
    "            generated_tokens = accelerator.pad_across_processes(\n",
    "                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "            )\n",
    "            labels = batch[\"labels\"]\n",
    "            # 如果我们没有填充到最大长度,我们需要填充标签\n",
    "            labels = accelerator.pad_across_processes(\n",
    "                batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id\n",
    "            )\n",
    "            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n",
    "            labels = accelerator.gather(labels).cpu().numpy()\n",
    "            # 替换标签中的 -100,因为我们无法解码它们\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            if isinstance(generated_tokens, tuple):\n",
    "                generated_tokens = generated_tokens[0]\n",
    "            decoded_preds = tokenizer.batch_decode(\n",
    "                generated_tokens, skip_special_tokens=True\n",
    "            )\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            decoded_preds, decoded_labels = postprocess_text(\n",
    "                decoded_preds, decoded_labels\n",
    "            )\n",
    "            rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "    # 计算评估的 loss\n",
    "    result = rouge_score.compute()\n",
    "    # 提取中位 ROUGE 分数\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    print(f\"Epoch {epoch}:\", result)\n",
    "    # 保存模型\n",
    "    output_dir = f\"../model/{model_name}-epoch{epoch}\"\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(\n",
    "        output_dir,\n",
    "        is_main_process=accelerator.is_main_process,\n",
    "        save_function=accelerator.save\n",
    "    )\n",
    "    # 同时保存训练状态\n",
    "    accelerator.save(\n",
    "        {\"epoch\": epoch, \"optimizer_state\": optimizer.state_dict(), \"lr_scheduler_state\": lr_scheduler.state_dict()}, \n",
    "        f\"{output_dir}/training_state.pt\"\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        print(f\"\\nModel saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用微调后的模型\n",
    "from transformers import pipeline\n",
    "# 将其替换成你自己的 checkpoint\n",
    "model_checkpoint = \"../model/mt5-small-finetuned-amazon-en-es-epoch0\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "def print_summary(idx):\n",
    "    review = books_dataset[\"test\"][idx][\"review_body\"]\n",
    "    title = books_dataset[\"test\"][idx][\"review_title\"]\n",
    "    summary = summarizer(books_dataset[\"test\"][idx][\"review_body\"])[0][\"summary_text\"]\n",
    "    print(f\"'>>> Review: {review}'\")\n",
    "    print(f\"\\n'>>> Title: {title}'\")\n",
    "    print(f\"\\n'>>> Summary: {summary}'\")\n",
    "# 使用测试集中部分样本，来感受一下生成摘要的质量\n",
    "print_summary(100)\n",
    "print_summary(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从头开始训练因果语言模型\n",
    "使用 Python 代码的一个数据集，来实现一行代码的补全，而不是直接生成完整的函数或类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def any_keyword_in_string(string, keywords):\n",
    "    for keyword in keywords:\n",
    "        if keyword in string:\n",
    "            return True\n",
    "    return False\n",
    "filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
    "example_1 = \"import numpy as np\"\n",
    "example_2 = \"import pandas as pd\"\n",
    "print(\n",
    "    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
